{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pybullet_data\n"
     ]
    }
   ],
   "source": [
    "import pybullet_data\n",
    "import pybullet as p\n",
    "import pybullet_industrial as pi\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "p.connect(p.GUI)  # Use p.DIRECT for headless\n",
    "#p.connect(p.DIRECT)\n",
    "data_path = pybullet_data.getDataPath()\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "print(pybullet_data.getDataPath())\n",
    "plane_id = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Get the path to the PyBullet data directory\n",
    "data_path = pybullet_data.getDataPath()\n",
    "\n",
    "# List the files in the PyBullet data directory\n",
    "files = os.getcwd()\n",
    "dirname = os.path.join(files,'robots')\n",
    "assets = os.path.join(files,'assets')\n",
    "\n",
    "urdf_file1 = os.path.join(dirname,'rb5_850e.urdf')\n",
    "asset1 = os.path.join(assets,'specimen.urdf')\n",
    "#endeffector = os.path.join(assets,'endeffector.urdf')\n",
    "endeffector = os.path.join(assets,\n",
    "                                'gripper_cylinder.urdf')\n",
    "\n",
    "start_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "\n",
    "cube1 = p.loadURDF(\"cube.urdf\", np.array(\n",
    "        [0, 0, 0.5]), useFixedBase=True)\n",
    "robot = pi.RobotBase(urdf_file1, np.array(\n",
    "        [0, 0, 1]), start_orientation)\n",
    "cube2 = p.loadURDF(\"cube.urdf\", np.array(\n",
    "        [1, 0, 0.5]), useFixedBase=True)\n",
    "specimen = p.loadURDF(asset1, np.array(\n",
    "[1.0, 0, 1.3]), useFixedBase=True)\n",
    "meshScale = [1, 1, 1]\n",
    "\n",
    "pi.draw_robot_frames(robot, life_time=0)\n",
    "\n",
    "tool = pi.EndeffectorTool(endeffector, np.array([2.0, 0, 1.5]), start_orientation, robot)\n",
    "p.setGravity(0, 0, -9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    position_list = [1,0.16175214925291828,0.49468914613294146,1.4254185043970449,-0.350008612250527,1.5717769960272012,-0.1634941307782464,0]\n",
    "    gripper_position = [0.5, 0, 1.1]\n",
    "    gripper_orientation = [1, 1, 1, 1]\n",
    "    #update_gripper(gripper_position, gripper_orientation)\n",
    "\n",
    "    pose_dict = {\n",
    "    \"base\": 0.16175214925291828,\n",
    "    \"shoulder\": 0.49468914613294146,\n",
    "    \"elbow\": 1.4254185043970449,\n",
    "    \"wrist1\": -0.3500086122505276,\n",
    "    \"wrist2\": 1.5717769960272012,\n",
    "    \"wrist3\": -0.1634941307782464\n",
    "\n",
    "    }\n",
    "\n",
    "    pi.RobotBase.reset_robot(robot,[0, 0, 1], start_orientation, position_list)\n",
    "    pi.RobotBase.set_joint_position(robot,pose_dict)\n",
    "    for i in range(1000):\n",
    "      p.stepSimulation()\n",
    "\n",
    "def check_collision(visualize=False):\n",
    "    contacts = p.getContactPoints() \n",
    "    contacts = [c for c in contacts if c[1] >= 4] # 로봇의 베이스나, 엔드이펙터와 플랜지사이의 충돌은 무시\n",
    "    if contacts:\n",
    "        print(\"TCP 링크가 무언가와 충돌 발생!\")\n",
    "        return True\n",
    "        for contact in contacts:\n",
    "            print(contact)\n",
    "    \n",
    "    if visualize:\n",
    "        for c in contacts:\n",
    "            pos_onB = c[6]\n",
    "            p.removeAllUserDebugItems()\n",
    "            p.addUserDebugPoints(\n",
    "                pointPositions=[pos_onB],\n",
    "                pointColorsRGB=[[1, 0, 0]],   # 빨간색\n",
    "                pointSize=50,\n",
    "                lifeTime=10\n",
    "            )\n",
    "\n",
    "def update_camera(camera_position, camera_orientation):\n",
    "    camera_id = None\n",
    "    for body_id in range(p.getNumBodies()):\n",
    "        body_name = p.getBodyInfo(body_id)[1].decode()\n",
    "        if \"camera\" in body_name:  # 카메라 URDF 파일명이 포함된 경우\n",
    "            camera_id = body_id\n",
    "            break\n",
    "    pos_offset = np.array([0.1, 0, 0.04])\n",
    "    ori_offset = np.array([0.5, 0.5, 0, 0])\n",
    "    p.resetBasePositionAndOrientation(camera_id, camera_position + pos_offset, camera_orientation + ori_offset)\n",
    "\n",
    "def update_specimen(specimen_position, specimen_orientation):\n",
    "    specimen_id = None\n",
    "    for body_id in range(p.getNumBodies()):\n",
    "        body_name = p.getBodyInfo(body_id)[1].decode()\n",
    "        if \"my_specimen\" in body_name:  # my_specimen URDF 파일명이 포함된 경우\n",
    "            specimen_id = body_id\n",
    "            break\n",
    "    p.resetBasePositionAndOrientation(specimen_id, specimen_position, specimen_orientation)\n",
    "\n",
    "def update_gripper(gripper_position, gripper_orientation):\n",
    "    gripper_id = None\n",
    "    for body_id in range(p.getNumBodies()):\n",
    "        body_name = p.getBodyInfo(body_id)[1].decode()\n",
    "        if \"gripper\" in body_name:  # gripper URDF 파일명이 포함된 경우\n",
    "            gripper_id = body_id\n",
    "            break\n",
    "    p.resetBasePositionAndOrientation(gripper_id, gripper_position, gripper_orientation)\n",
    "\n",
    "def show_point(tool_pos):\n",
    "    p.addUserDebugPoints(\n",
    "    pointPositions=[tool_pos],\n",
    "    pointColorsRGB=[[1, 0, 1]],  \n",
    "    pointSize=10,\n",
    "    lifeTime=1\n",
    ")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize()\n",
    "\n",
    "urdf_file3 = os.path.join(assets, 'camera.urdf')\n",
    "camera_parameters = {'width': 480, 'height': 240, 'fov': 60,\n",
    "                        'aspect ratio': 1, 'near plane distance': 0.01, 'far plane distance': 2}\n",
    "camera_orientation = p.getQuaternionFromEuler([1.57, 0, 1.57])\n",
    "camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0.1])\n",
    "camera = pi.Camera(urdf_file3, camera_position,\n",
    "                camera_orientation, camera_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pose = np.array([0, 0, 0.5])\n",
    "set_pose = np.array([0.860, 0, 0.81])\n",
    "base_orientation = np.array([1.57, 0, 1.57])\n",
    "\n",
    "target_pose = base_pose + set_pose\n",
    "\n",
    "\n",
    "specimen_position = np.array([1.0, 0, 1.30])\n",
    "ring_offset = np.array([0, 0, 0.015])\n",
    "specimen_orientation_euler = np.array([0, 0, 0])\n",
    "specimen_orientation = p.getQuaternionFromEuler(specimen_orientation_euler)\n",
    "update_specimen(specimen_position, specimen_orientation)\n",
    "\n",
    "for _ in range(24 * 1):  # 240 Hz simulation frequency\n",
    "    tool.set_tool_pose(target_pose, p.getQuaternionFromEuler(base_orientation))\n",
    "    img = camera.get_image()\n",
    "    p.stepSimulation()\n",
    "    camera_orientation = p.getQuaternionFromEuler([1.57, 0, 1.57])\n",
    "    camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0])\n",
    "    update_camera(camera_position, camera_orientation)\n",
    "    show_point(tool.get_tool_pose()[0]+np.array([0.135,0,0]))\n",
    "    show_point(specimen_position+ring_offset)\n",
    "    #time.sleep(1./24.)\n",
    "\n",
    "check_collision(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │ input_states[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93184</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23,855,360</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,542</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m480\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m119\u001b[0m,   │      \u001b[38;5;34m6,176\u001b[0m │ input_states[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m58\u001b[0m,    │     \u001b[38;5;34m32,832\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93184\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │ \u001b[38;5;34m23,855,360\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │      \u001b[38;5;34m1,542\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,932,838</span> (91.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,932,838\u001b[0m (91.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,932,838</span> (91.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,932,838\u001b[0m (91.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93184</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,855,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m480\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m119\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93184\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m23,855,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,931,553</span> (91.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,931,553\u001b[0m (91.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,931,553</span> (91.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,931,553\u001b[0m (91.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "5\n",
      "[0.14738965 0.16680458 0.14483137 0.17152305 0.17178258 0.19766875]\n",
      "reward:  11.59672502978044\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[0.14702684 0.1678355  0.14671405 0.17207892 0.17101172 0.19533302]\n",
      "reward:  9.544449804094857\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[0.14770856 0.16617456 0.14759198 0.17088884 0.17230184 0.19533415]\n",
      "reward:  7.790401773231707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "2\n",
      "[0.14767788 0.16588552 0.14568244 0.17153358 0.17175327 0.19746725]\n",
      "reward:  7.559776082191911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "2\n",
      "[0.14812665 0.16752279 0.14524229 0.1715415  0.17112218 0.19644462]\n",
      "reward:  7.008831575892479\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "5\n",
      "[0.14925723 0.16653852 0.14605801 0.1717059  0.17124487 0.19519544]\n",
      "reward:  6.0238434098599205\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "2\n",
      "[0.14813349 0.16631718 0.145279   0.1721143  0.17050669 0.1976493 ]\n",
      "reward:  5.556710874521681\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.14850016 0.1658881  0.14545038 0.17208439 0.17086327 0.19721372]\n",
      "reward:  5.0518319883234675\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "2\n",
      "[0.14829867 0.16584873 0.14536703 0.17205334 0.17109618 0.19733603]\n",
      "reward:  4.567168770715318\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "3\n",
      "[0.14767174 0.16681921 0.14520359 0.17141733 0.17100702 0.19788116]\n",
      "reward:  5.0022218665655265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_states']\n",
      "Received: inputs=Tensor(shape=(10, 240, 480, 3))\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_states']\n",
      "Received: inputs=Tensor(shape=(None, 240, 480, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "2\n",
      "[0.00728823 0.01013125 0.23817153 0.04085829 0.02929092 0.6742598 ]\n",
      "reward:  12.174366003061625\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.00868408 0.01196556 0.24594562 0.04475833 0.0330813  0.65556514]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "5\n",
      "[0.00851231 0.01159409 0.24418922 0.0441459  0.03257785 0.6589806 ]\n",
      "reward:  9.164853899083207\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.00861056 0.01167234 0.24392617 0.04447377 0.03235601 0.65896124]\n",
      "reward:  7.576152814275589\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.0084083  0.0115296  0.2432415  0.04405533 0.03204433 0.660721  ]\n",
      "reward:  6.336224517125618\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[0.00829035 0.01140194 0.24273887 0.04383542 0.03175819 0.6619753 ]\n",
      "reward:  5.394541946783456\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "2\n",
      "[0.00825857 0.01135355 0.24255301 0.04385383 0.0317929  0.6621882 ]\n",
      "reward:  5.1742252343850925\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[0.00828601 0.01139937 0.24238642 0.0438536  0.03181907 0.6622555 ]\n",
      "reward:  4.5526985222135306\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "3\n",
      "[0.00830415 0.01142739 0.24184494 0.04403377 0.0319051  0.6624847 ]\n",
      "reward:  4.6787454653644645\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[0.00832591 0.01144594 0.24221703 0.0441091  0.03195423 0.6619477 ]\n",
      "reward:  4.148307347809415\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "2\n",
      "[1.7485539e-04 3.3155756e-04 9.4544351e-02 8.8782394e-03 6.6871452e-04\n",
      " 8.9540231e-01]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[2.53047678e-04 4.68297920e-04 1.04354024e-01 1.06981844e-02\n",
      " 9.17592610e-04 8.83308828e-01]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[2.4072851e-04 4.4345993e-04 1.0257754e-01 1.0457200e-02 8.8024698e-04\n",
      " 8.8540083e-01]\n",
      "reward:  9.16485389905902\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "5\n",
      "[2.4253494e-04 4.4490851e-04 1.0243634e-01 1.0474674e-02 8.7378855e-04\n",
      " 8.8552767e-01]\n",
      "reward:  7.57615281419158\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "5\n",
      "[2.3365473e-04 4.3226956e-04 1.0154360e-01 1.0272887e-02 8.4924896e-04\n",
      " 8.8666826e-01]\n",
      "reward:  6.3362245170462\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[2.27544559e-04 4.22610698e-04 1.00907214e-01 1.01745855e-02\n",
      " 8.30487872e-04 8.87437522e-01]\n",
      "reward:  5.394541946739914\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "5\n",
      "[2.26837947e-04 4.21191682e-04 1.00805044e-01 1.01770852e-02\n",
      " 8.31816928e-04 8.87538016e-01]\n",
      "reward:  4.673460828139819\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[2.29064768e-04 4.25666280e-04 1.00751795e-01 1.02341874e-02\n",
      " 8.38649226e-04 8.87520611e-01]\n",
      "reward:  4.111032894171858\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[2.3368042e-04 4.3368805e-04 1.0117751e-01 1.0352395e-02 8.5333240e-04\n",
      " 8.8694930e-01]\n",
      "reward:  3.66345612146021\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[2.3843476e-04 4.4220011e-04 1.0164440e-01 1.0464017e-02 8.6745480e-04\n",
      " 8.8634354e-01]\n",
      "reward:  3.3004869923050593\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[7.6591796e-06 9.9095796e-06 5.5952247e-02 5.1899115e-04 7.5753174e-05\n",
      " 9.4343543e-01]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[1.20560726e-05 1.54920745e-05 6.22923225e-02 6.95423980e-04\n",
      " 1.08890075e-04 9.36875701e-01]\n",
      "reward:  9.54444980405987\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[1.20432760e-05 1.53367746e-05 6.24070838e-02 6.84712664e-04\n",
      " 1.09290355e-04 9.36771631e-01]\n",
      "reward:  7.790401773227033\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[1.15513385e-05 1.47432820e-05 6.16057627e-02 6.69217436e-04\n",
      " 1.04907020e-04 9.37593818e-01]\n",
      "reward:  6.4622473715317845\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "5\n",
      "[1.10313622e-05 1.41480914e-05 6.08684383e-02 6.55165408e-04\n",
      " 1.01157515e-04 9.38350141e-01]\n",
      "reward:  5.4730822898678815\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[1.10348083e-05 1.41760156e-05 6.09316044e-02 6.55579555e-04\n",
      " 1.01416925e-04 9.38286185e-01]\n",
      "reward:  4.725167919153875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "2\n",
      "[1.11822865e-05 1.43735351e-05 6.10317960e-02 6.60885067e-04\n",
      " 1.02638849e-04 9.38179076e-01]\n",
      "reward:  4.664847483843465\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[1.13024907e-05 1.45251461e-05 6.11652881e-02 6.65266067e-04\n",
      " 1.03627266e-04 9.38039958e-01]\n",
      "reward:  4.131018192156803\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[1.1487534e-05 1.4755280e-05 6.1380483e-02 6.7264319e-04 1.0491843e-04\n",
      " 9.3781561e-01]\n",
      "reward:  3.6774347162531575\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[1.1813536e-05 1.5176042e-05 6.1720349e-02 6.8481278e-04 1.0725019e-04\n",
      " 9.3746054e-01]\n",
      "reward:  3.311963767179804\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[6.085606e-06 5.138758e-06 6.344167e-02 2.445571e-04 6.605822e-05\n",
      " 9.362365e-01]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[9.6745880e-06 8.2456190e-06 7.0555195e-02 3.3737603e-04 9.5701995e-05\n",
      " 9.2899394e-01]\n",
      "reward:  9.544449804107344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "2\n",
      "[9.6718868e-06 8.1786275e-06 7.0555545e-02 3.3292457e-04 9.6012736e-05\n",
      " 9.2899764e-01]\n",
      "reward:  9.147765302220417\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[9.7748016e-06 8.2783390e-06 7.0327245e-02 3.3683519e-04 9.6308911e-05\n",
      " 9.2922145e-01]\n",
      "reward:  7.6220858191406355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "5\n",
      "[9.1155116e-06 7.7360710e-06 6.9071099e-02 3.2124075e-04 9.1005837e-05\n",
      " 9.3049979e-01]\n",
      "reward:  6.367800283690669\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[8.7225508e-06 7.4156283e-06 6.8674728e-02 3.1249039e-04 8.7868444e-05\n",
      " 9.3090880e-01]\n",
      "reward:  5.418532861299974\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[8.7364579e-06 7.4133827e-06 6.8690509e-02 3.1372946e-04 8.8184795e-05\n",
      " 9.3089145e-01]\n",
      "reward:  4.691911855517176\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[8.946142e-06 7.608547e-06 6.887333e-02 3.190297e-04 8.989936e-05\n",
      " 9.307012e-01]\n",
      "reward:  4.125545285998952\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[9.1989850e-06 7.8277944e-06 6.9250889e-02 3.2536624e-04 9.1930597e-05\n",
      " 9.3031472e-01]\n",
      "reward:  3.675104729531495\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[9.4689694e-06 8.0669306e-06 6.9615908e-02 3.3196792e-04 9.4025359e-05\n",
      " 9.2994046e-01]\n",
      "reward:  3.3100089908689676\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "5\n",
      "[1.1144579e-04 8.6274333e-05 1.9759801e-01 1.3956816e-03 7.1597198e-04\n",
      " 8.0009264e-01]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "2\n",
      "[1.5498402e-04 1.2128763e-04 2.0597623e-01 1.7666122e-03 9.2656765e-04\n",
      " 7.9105431e-01]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[1.5762771e-04 1.2292391e-04 2.0640925e-01 1.7730236e-03 9.4401540e-04\n",
      " 7.9059321e-01]\n",
      "reward:  9.215001721308408\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[1.5647715e-04 1.2184177e-04 2.0665173e-01 1.7685504e-03 9.2821475e-04\n",
      " 7.9037321e-01]\n",
      "reward:  7.613700256260402\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[1.50113090e-04 1.17234544e-04 2.05167353e-01 1.71875872e-03\n",
      " 8.97936639e-04 7.91948557e-01]\n",
      "reward:  6.3650219993292465\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[1.4485078e-04 1.1303603e-04 2.0444404e-01 1.6794821e-03 8.7761087e-04\n",
      " 7.9274094e-01]\n",
      "reward:  5.416478458313586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[1.4520939e-04 1.1331070e-04 2.0429753e-01 1.6862552e-03 8.8073639e-04\n",
      " 7.9287690e-01]\n",
      "reward:  4.690445949869298\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[1.4776186e-04 1.1546521e-04 2.0458415e-01 1.7050320e-03 8.9307048e-04\n",
      " 7.9255456e-01]\n",
      "reward:  4.124443107282737\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[1.5061023e-04 1.1781318e-04 2.0507520e-01 1.7299474e-03 9.0694643e-04\n",
      " 7.9201943e-01]\n",
      "reward:  3.6742503046332713\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[1.5383906e-04 1.2048878e-04 2.0551969e-01 1.7565496e-03 9.2206884e-04\n",
      " 7.9152733e-01]\n",
      "reward:  3.309329211775883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[1.3411694e-10 1.6214509e-10 3.2514411e-06 2.3362267e-07 3.7716639e-09\n",
      " 9.9999654e-01]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[3.2492789e-10 3.8935455e-10 5.3253498e-06 4.2336069e-07 7.9861611e-09\n",
      " 9.9999416e-01]\n",
      "reward:  9.544449804107344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[3.3059347e-10 3.9073680e-10 5.3916542e-06 4.2357070e-07 8.1330258e-09\n",
      " 9.9999416e-01]\n",
      "reward:  7.79040177323886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[2.9707167e-10 3.5306275e-10 5.0598078e-06 3.9686842e-07 7.4117072e-09\n",
      " 9.9999452e-01]\n",
      "reward:  6.462247371534671\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[2.7767719e-10 3.3236461e-10 4.8768466e-06 3.8153556e-07 6.9919204e-09\n",
      " 9.9999475e-01]\n",
      "reward:  5.473082289875404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[2.7573077e-10 3.3138850e-10 4.8442503e-06 3.8010754e-07 6.9543344e-09\n",
      " 9.9999475e-01]\n",
      "reward:  4.7251679191474105\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[2.8603458e-10 3.4412057e-10 4.9468658e-06 3.8933862e-07 7.1895130e-09\n",
      " 9.9999464e-01]\n",
      "reward:  4.146724977144796\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[3.0082470e-10 3.6213507e-10 5.0865292e-06 4.0355670e-07 7.5092368e-09\n",
      " 9.9999452e-01]\n",
      "reward:  3.6890935367122335\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[3.1734712e-10 3.8214362e-10 5.2342011e-06 4.1839570e-07 7.8590645e-09\n",
      " 9.9999428e-01]\n",
      "reward:  3.3195251808341153\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[3.3586814e-10 4.0379194e-10 5.4031070e-06 4.3408460e-07 8.2424991e-09\n",
      " 9.9999416e-01]\n",
      "TCP 링크가 무언가와 충돌 발생!\n",
      "reward:  -396.8891122838272\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[7.1694713e-18 1.7696753e-17 1.5337320e-11 5.6753127e-12 1.8348036e-15\n",
      " 1.0000000e+00]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[3.3624452e-17 7.9545255e-17 4.0295784e-11 1.5562929e-11 6.9029822e-15\n",
      " 1.0000000e+00]\n",
      "reward:  9.544449804107344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[3.4986448e-17 8.1265287e-17 4.1533298e-11 1.5769101e-11 7.1499000e-15\n",
      " 1.0000000e+00]\n",
      "reward:  7.79040177323886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[2.8980230e-17 6.8133980e-17 3.6835084e-11 1.4064871e-11 6.0685959e-15\n",
      " 1.0000000e+00]\n",
      "reward:  6.462247371534671\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[2.52579203e-17 6.01922788e-17 3.38950604e-11 1.29430616e-11\n",
      " 5.38773009e-15 1.00000000e+00]\n",
      "reward:  5.473082289875404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[2.4935674e-17 5.9844274e-17 3.3534817e-11 1.2876799e-11 5.3358396e-15\n",
      " 1.0000000e+00]\n",
      "reward:  4.7251679191474105\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[2.6122397e-17 6.2734205e-17 3.4568064e-11 1.3273572e-11 5.5679440e-15\n",
      " 1.0000000e+00]\n",
      "reward:  4.146724977144796\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[2.8492646e-17 6.8375603e-17 3.6490196e-11 1.4083743e-11 6.0044878e-15\n",
      " 1.0000000e+00]\n",
      "reward:  3.6890935367122335\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[3.1163441e-17 7.4637546e-17 3.8600557e-11 1.4935183e-11 6.4828963e-15\n",
      " 1.0000000e+00]\n",
      "reward:  3.3195251808341153\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[3.4374202e-17 8.1990189e-17 4.1034700e-11 1.5907914e-11 7.0442352e-15\n",
      " 1.0000000e+00]\n",
      "TCP 링크가 무언가와 충돌 발생!\n",
      "reward:  -396.88911555188537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[9.1816361e-22 4.1110012e-21 1.7972413e-14 2.0964843e-14 7.3961760e-19\n",
      " 1.0000000e+00]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[6.1133288e-21 2.5667678e-20 6.1397827e-14 7.1712913e-14 3.7786222e-18\n",
      " 1.0000000e+00]\n",
      "reward:  9.544449804107344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[6.3830511e-21 2.6321112e-20 6.3605412e-14 7.2917014e-14 3.9231696e-18\n",
      " 1.0000000e+00]\n",
      "reward:  7.79040177323886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[5.0766809e-21 2.1250373e-20 5.4722533e-14 6.3290763e-14 3.2153149e-18\n",
      " 1.0000000e+00]\n",
      "reward:  6.462247371534668\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[4.2809008e-21 1.8178976e-20 4.9104723e-14 5.6937277e-14 2.7749170e-18\n",
      " 1.0000000e+00]\n",
      "reward:  5.473082289876748\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[4.2309460e-21 1.8083320e-20 4.8560229e-14 5.6699620e-14 2.7501522e-18\n",
      " 1.0000000e+00]\n",
      "reward:  4.725167919156179\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[4.4873046e-21 1.9168547e-20 5.0488873e-14 5.8902746e-14 2.8999234e-18\n",
      " 1.0000000e+00]\n",
      "reward:  4.146724977160175\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[4.9925625e-21 2.1286071e-20 5.4100885e-14 6.3289434e-14 3.1820770e-18\n",
      " 1.0000000e+00]\n",
      "reward:  3.6890935367305233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[5.5666735e-21 2.3656456e-20 5.8070234e-14 6.7939293e-14 3.4935216e-18\n",
      " 1.0000000e+00]\n",
      "reward:  3.319525180841046\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[6.2760176e-21 2.6519052e-20 6.2757403e-14 7.3356712e-14 3.8683184e-18\n",
      " 1.0000000e+00]\n",
      "TCP 링크가 무언가와 충돌 발생!\n",
      "reward:  -396.8902089991666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[2.1311226e-23 1.2593778e-22 1.0176170e-15 2.0105305e-15 2.7946623e-20\n",
      " 1.0000000e+00]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[1.6431966e-22 9.0099568e-22 3.8861020e-15 7.5371787e-15 1.6223340e-19\n",
      " 1.0000000e+00]\n",
      "reward:  9.54444980405987\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[1.7194228e-22 9.2497311e-22 4.0323300e-15 7.6700197e-15 1.6867544e-19\n",
      " 1.0000000e+00]\n",
      "reward:  7.790401773227033\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[1.3452114e-22 7.3562933e-22 3.4262436e-15 6.5908930e-15 1.3627640e-19\n",
      " 1.0000000e+00]\n",
      "reward:  6.4622473715317845\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[1.1183463e-22 6.2143130e-22 3.0438550e-15 5.8783633e-15 1.1623391e-19\n",
      " 1.0000000e+00]\n",
      "reward:  5.473082289881717\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[1.1055870e-22 6.1832180e-22 3.0098280e-15 5.8553118e-15 1.1522559e-19\n",
      " 1.0000000e+00]\n",
      "reward:  4.725167919157552\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[1.1785535e-22 6.5851402e-22 3.1412443e-15 6.1023028e-15 1.2203627e-19\n",
      " 1.0000000e+00]\n",
      "reward:  4.146724977161966\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[1.3223819e-22 7.3701126e-22 3.3877866e-15 6.5916727e-15 1.3490367e-19\n",
      " 1.0000000e+00]\n",
      "reward:  3.689093536735671\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[1.4863572e-22 8.2521520e-22 3.6588808e-15 7.1117623e-15 1.4914771e-19\n",
      " 1.0000000e+00]\n",
      "reward:  3.3195251808366475\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[1.6911327e-22 9.3287162e-22 3.9816135e-15 7.7223367e-15 1.6645549e-19\n",
      " 1.0000000e+00]\n",
      "TCP 링크가 무언가와 충돌 발생!\n",
      "reward:  -396.8891132973631\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[4.989337e-24 3.289748e-23 3.340177e-16 8.144377e-16 7.896943e-21\n",
      " 1.000000e+00]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[4.0703942e-23 2.4799389e-22 1.3317672e-15 3.1626103e-15 4.8152972e-20\n",
      " 1.0000000e+00]\n",
      "reward:  9.544449804107344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[4.2626942e-23 2.5469262e-22 1.3827336e-15 3.2193209e-15 5.0091421e-20\n",
      " 1.0000000e+00]\n",
      "reward:  7.79040177323886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[3.3141842e-23 2.0140776e-22 1.1693238e-15 2.7558801e-15 4.0251436e-20\n",
      " 1.0000000e+00]\n",
      "reward:  6.462247371534668\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "5\n",
      "[2.7404375e-23 1.6933018e-22 1.0348061e-15 2.4499153e-15 3.4176977e-20\n",
      " 1.0000000e+00]\n",
      "reward:  5.473082289879998\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[2.7094816e-23 1.6850025e-22 1.0232224e-15 2.4405874e-15 3.3884113e-20\n",
      " 1.0000000e+00]\n",
      "reward:  4.725167919148611\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[2.8940148e-23 1.7978063e-22 1.0694805e-15 2.5466924e-15 3.5948164e-20\n",
      " 1.0000000e+00]\n",
      "reward:  4.146724977157277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[3.2577655e-23 2.0181923e-22 1.1563049e-15 2.7565425e-15 3.9851752e-20\n",
      " 1.0000000e+00]\n",
      "reward:  3.6890935367315585\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[3.6731169e-23 2.2661995e-22 1.2518337e-15 2.9798278e-15 4.4178886e-20\n",
      " 1.0000000e+00]\n",
      "reward:  3.319525180851894\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[4.1939509e-23 2.5701747e-22 1.3659051e-15 3.2427267e-15 4.9455927e-20\n",
      " 1.0000000e+00]\n",
      "TCP 링크가 무언가와 충돌 발생!\n",
      "reward:  -396.89021336393665\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[2.9013862e-24 1.9934237e-23 2.2017592e-16 5.8121596e-16 4.9272871e-21\n",
      " 1.0000000e+00]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[2.4173852e-23 1.5323290e-22 8.9213353e-16 2.2867888e-15 3.0600884e-20\n",
      " 1.0000000e+00]\n",
      "reward:  9.54444980405987\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[2.5323061e-23 1.5739119e-22 9.2647670e-16 2.3280521e-15 3.1838586e-20\n",
      " 1.0000000e+00]\n",
      "reward:  7.790401773227033\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[1.9642862e-23 1.2420211e-22 7.8211142e-16 1.9901035e-15 2.5533007e-20\n",
      " 1.0000000e+00]\n",
      "reward:  6.4622473715317845\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[1.6210005e-23 1.0423697e-22 6.9113579e-16 1.7670259e-15 2.1643559e-20\n",
      " 1.0000000e+00]\n",
      "reward:  5.473082289881717\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[1.60278155e-23 1.03730425e-22 6.83404393e-16 1.76038540e-15\n",
      " 2.14593221e-20 1.00000000e+00]\n",
      "reward:  4.725167919157552\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[1.7131697e-23 1.1074866e-22 7.1469236e-16 1.8377528e-15 2.2780674e-20\n",
      " 1.0000000e+00]\n",
      "reward:  4.146724977161966\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[1.9308548e-23 1.2446582e-22 7.7343907e-16 1.9907110e-15 2.5281307e-20\n",
      " 1.0000000e+00]\n",
      "reward:  3.6890935367360385\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[2.1795727e-23 1.3991082e-22 8.3808828e-16 2.1535313e-15 2.8054612e-20\n",
      " 1.0000000e+00]\n",
      "reward:  3.3195251808402384\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[2.4918960e-23 1.5886906e-22 9.1537559e-16 2.3454342e-15 3.1441374e-20\n",
      " 1.0000000e+00]\n",
      "TCP 링크가 무언가와 충돌 발생!\n",
      "reward:  -396.89021135906313\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[2.3754799e-24 1.6560456e-23 1.8890130e-16 5.1310710e-16 4.1405265e-21\n",
      " 1.0000000e+00]\n",
      "reward:  10.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.client import device_lib\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "LOSS_CLIPPING = 0.3\n",
    "\n",
    "class ModelParam:\n",
    "    pass\n",
    "\n",
    "def AI_init():\n",
    "    model_param = ModelParam()\n",
    "    # state and action space\n",
    "    model_param.state_size = 4     # [pos,vel,angle,angular_vel] \n",
    "    model_param.img_height = 240\n",
    "    model_param.img_width = 480\n",
    "    model_param.img_channels = 3\n",
    "    model_param.action_size = 6   # [x+,x-,y+,y-,z+,z-,roll+,roll-,pitch+,pitch-,yaw+,yaw-]\n",
    "    model_param.value_size = 1\n",
    "\n",
    "    # AI hyperparameters\n",
    "    model_param.learning_rate_actor = 0.0001\n",
    "    model_param.learning_rate_critic = 0.0001\n",
    "    model_param.epochs_cnt = 10\n",
    "    model_param.discount_rate = 0.90\n",
    "    model_param.smooth_rate = 0.95\n",
    "    model_param.penalty = -400\n",
    "    model_param.episode_num = 50\n",
    "    model_param.mini_batch_step_size = 10        \n",
    "    model_param.node_num = 256     # used after flatten\n",
    "\n",
    "    # Debug variables\n",
    "    model_param.moving_avg_size = 20\n",
    "\n",
    "    # model Initialization\n",
    "    model_param.model_actor = build_model_actor(model_param)\n",
    "    model_param.model_critic = build_model_critic(model_param)\n",
    "\n",
    "    # reward buffer\n",
    "    model_param.reward_list= []\n",
    "    model_param.count_list = []\n",
    "    model_param.moving_avg_list = []\n",
    "\n",
    "    model_param.states, model_param.states_next, model_param.action_matrixs = [],[],[]\n",
    "    model_param.dones, model_param.action_probs, model_param.rewards = [],[],[]\n",
    "\n",
    "    # dummy data\n",
    "    model_param.DUMMY_ACTION_MATRIX = np.zeros((1,1,model_param.action_size))\n",
    "    model_param.DUMMY_ADVANTAGE = np.zeros((1,1,model_param.value_size))\n",
    "    return model_param\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages = in_datas[0], in_datas[1], in_datas[2]\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self([states, action_matrixs, advantages], training=True)\n",
    "                new_policy = K.max(action_matrixs*y_pred, axis=-1)   \n",
    "                old_policy = K.max(action_matrixs*out_action_probs, axis=-1)   \n",
    "                r = new_policy/(old_policy)\n",
    "                clipped = K.clip(r, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)\n",
    "                loss = -K.minimum(r*advantages, clipped*advantages)\n",
    "            \n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            return {\"loss\": loss}\n",
    "        \n",
    "def build_model_actor(model_param):\n",
    "     # 1) The main input: an RGB image\n",
    "    input_states = Input(shape=(model_param.img_height, model_param.img_width, model_param.img_channels),\n",
    "                         name='input_states')\n",
    "\n",
    "    # 2) Additional inputs (same as your old code):\n",
    "    input_action_matrixs = Input(shape=(1, model_param.action_size), name='input_action_matrixs')\n",
    "    input_advantages = Input(shape=(1, model_param.value_size), name='input_advantages')\n",
    "\n",
    "    # 3) Build a small CNN\n",
    "    x = Conv2D(32, kernel_size=8, strides=4, activation='relu')(input_states)\n",
    "    x = Conv2D(64, kernel_size=4, strides=2, activation='relu')(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(model_param.node_num, activation='relu')(x)\n",
    "\n",
    "    # 4) Output: policy over model_param.action_size\n",
    "    out_actions = Dense(model_param.action_size, activation='softmax', name='output')(x)\n",
    "\n",
    "    # 5) Create a MyModel with multi-input\n",
    "    model = MyModel(inputs=[input_states, input_action_matrixs, input_advantages],\n",
    "                    outputs=out_actions)\n",
    "    model.compile(optimizer=Adam(learning_rate=model_param.learning_rate_actor))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_model_critic(model_param):\n",
    "    # Takes image input: (height, width, channels)\n",
    "    input_states = Input(shape=(model_param.img_height, model_param.img_width, model_param.img_channels),\n",
    "                         name='input_states')\n",
    "\n",
    "    x = Conv2D(32, kernel_size=8, strides=4, activation='relu')(input_states)\n",
    "    x = Conv2D(64, kernel_size=4, strides=2, activation='relu')(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(model_param.node_num, activation='relu')(x)\n",
    "\n",
    "    out_values = Dense(model_param.value_size, activation='linear', name='output')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "    model.compile(optimizer=Adam(learning_rate=model_param.learning_rate_critic),\n",
    "                  loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def norm(pointA, pointB):\n",
    "    return np.sqrt((pointA[0] - pointB[0])**2 + (pointA[1] - pointB[1])**2 + (pointA[2] - pointB[2])**2)\n",
    "\n",
    "def get_state(prv_states):\n",
    "    # init states[4]\n",
    "    dt = 1/240\n",
    "    states = np.zeros(4)\n",
    "    target_position = specimen_position+ring_offset\n",
    "    #print(target_position)\n",
    "    end_effector_position = tool.get_tool_pose()[0]+np.array([0.135,0,0])\n",
    "    #print(end_effector_position)\n",
    "    end_effector_orientation = p.getEulerFromQuaternion(tool.get_tool_pose()[1]) + np.array([-np.pi/2,0,-np.pi/2])\n",
    "    states[0] = norm(end_effector_position,target_position) # pos diff\n",
    "    states[1] = (prv_states[0]-states[0])/dt # velocity\n",
    "    states[2] = norm(end_effector_orientation,specimen_orientation_euler)# angle diff\n",
    "    states[3] = (prv_states[2]-states[2])/dt # angular velocity\n",
    "    #print states in a row\n",
    "    print(\"pos diff: \",states[0],\" velocity: \",states[1],\" angle diff: \",states[2],\" angular velocity: \",states[3])\n",
    "    print(\"angle: \",end_effector_orientation, \"angle_specimen\"  ,specimen_orientation_euler)\n",
    "    return states\n",
    "\n",
    "def train(model_param):\n",
    "    for episode in range(model_param.episode_num):\n",
    "        #initialize()\n",
    "        #model_param.states = np.zeros(4)\n",
    "        #model_param.states = get_state(model_param.states)\n",
    "        count, reward_tot = make_memory(model_param)\n",
    "        train_mini_batch(model_param)\n",
    "        clear_memory(model_param)\n",
    "\n",
    "        if count < model_param.episode_num:\n",
    "            reward_tot = reward_tot-model_param.penalty\n",
    "\n",
    "        model_param.reward_list.append(reward_tot)\n",
    "        model_param.count_list.append(count)\n",
    "        model_param.moving_avg_list.append(moving_avg(model_param.count_list,model_param.moving_avg_size))                \n",
    "            \n",
    "        if(episode % 10 == 0):\n",
    "            print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, model_param.moving_avg_list[-1], np.mean(model_param.reward_list)))\n",
    "    \n",
    "    model_param.save_model()\n",
    "def make_memory(model_param):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        base_pose = np.array([0, 0, 0.5])\n",
    "        reward = np.zeros(model_param.value_size)\n",
    "        #advantage = np.zeros(model_param.value_size)\n",
    "        #target = np.zeros(model_param.value_size)\n",
    "        action_matrix = np.zeros(model_param.action_size)\n",
    "        action_unit = np.zeros(model_param.action_size)\n",
    "        done = False\n",
    "        initialize()\n",
    "        prev_pos_reward = 0\n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            #state_t = np.reshape(state,[1, 1, model_param.state_size])\n",
    "            #action_matrix_t = np.reshape(action_matrix,[1, 1, model_param.action_size])\n",
    "\n",
    "            # 1. take a picture\n",
    "            img = camera.get_image()             # 카메라에서 이미지 획득\n",
    "            img = img.astype(np.float32) / 255.0   # float32 변환 및 0~1 정규화\n",
    "            rgb = img[:, :, :3]\n",
    "            rgb = np.expand_dims(rgb, axis=0)      # (1, H, W, 3) 모양으로 확장\n",
    "            model_param.states.append(rgb)\n",
    "            \n",
    "            # 2. calculate probabilty and predict action\n",
    "            action_prob = model_param.model_actor.predict([rgb, model_param.DUMMY_ACTION_MATRIX, model_param.DUMMY_ADVANTAGE])[0]\n",
    "            action = np.random.choice(model_param.action_size, 1, p=action_prob)[0]\n",
    "            action_matrix = np.zeros(model_param.action_size) #초기화\n",
    "            action_matrix[action] = 1 # 하나를 on 으로 만듬\n",
    "            print(action)\n",
    "            print(action_prob)\n",
    "            \n",
    "            # 3. move\n",
    "            # 3.1 robot move\n",
    "            action_unit = [0.05,-0.05,0.05,-0.05,0.05,-0.05] # +- 순서로 액션 을 정함\n",
    "            # action 위치 외 나머지는 0으로 초기화\n",
    "            action_vector = np.zeros_like(action_unit)\n",
    "            action_vector[action] = action_unit[action]  # 선택된 액션만 값을 유지\n",
    "\n",
    "            # 선택된 action에 따라 위치 업데이트 (회전은 별도로 처리)\n",
    "            action_pose = np.array(tool.get_tool_pose()[0])\n",
    "            if action in [0, 1]:\n",
    "                # x축 변경\n",
    "                action_pose[0] += action_unit[action]\n",
    "            elif action in [2, 3]:\n",
    "                # y축 변경\n",
    "                action_pose[1] += action_unit[action]\n",
    "            elif action in [4, 5]:\n",
    "                # z축 변경\n",
    "                action_pose[2] += action_unit[action]\n",
    "            \n",
    "            \n",
    "            # 3.2 camera move    \n",
    "            for i in range(10): # 10번 반복\n",
    "                tool.set_tool_pose(action_pose, p.getQuaternionFromEuler(base_orientation)) # 액션을 적용\n",
    "                camera_orientation = tool.get_tool_pose()[1] + p.getQuaternionFromEuler([1.57, 0, 1.57]) \n",
    "                camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0])\n",
    "                update_camera(camera_position, camera_orientation)    # 카메라 위치 업데이트\n",
    "                p.stepSimulation()# step simulation\n",
    "           #p.stepSimulation()# step simulation\n",
    "\n",
    "            img_next = camera.get_image()  # 액션 이후 이미지 상태\n",
    "            img_next_uint8 = img_next.astype(np.uint8)\n",
    "            rgb_next = img_next_uint8[:, :, :3]\n",
    "            rgb_next = np.expand_dims(rgb_next, axis=0)\n",
    "\n",
    "            # 다음 이미지 상태를 states_next에 저장\n",
    "            model_param.states_next.append(rgb_next)\n",
    "\n",
    "            # 3.3 check collision\n",
    "            is_collision = check_collision(visualize=False)\n",
    "\n",
    "            # 4. calculate reward\n",
    "            end_effector_position = tool.get_tool_pose()[0]+np.array([0.135,0,0])\n",
    "            specimen_position = np.array([1.0, 0, 1.30])\n",
    "            ring_offset = np.array([0, 0, 0.015])\n",
    "            target_position = specimen_position+ring_offset\n",
    "            judging_criterion = norm(end_effector_position,target_position) # pos difference\n",
    "            epsilon = 1e-6\n",
    "            if 1.0/(judging_criterion + epsilon) > prev_pos_reward:\n",
    "                pos_reward = 1.0/(judging_criterion + epsilon)\n",
    "                prev_pos_reward = pos_reward\n",
    "            else:\n",
    "                pos_reward = np.clip(1.0/(judging_criterion + epsilon), -10, 10)\n",
    "\n",
    "            angle_reward = 0\n",
    "            if is_collision:\n",
    "                collision_reward = model_param.penalty\n",
    "            else:\n",
    "                collision_reward = 0\n",
    "\n",
    "            reward = pos_reward + angle_reward + collision_reward\n",
    "            print(\"reward: \", reward)\n",
    "\n",
    "            # 5. judge if finished or not\n",
    "            if judging_criterion < 0.02:\n",
    "                reward = reward - model_param.penalty\n",
    "                done = True\n",
    "               \n",
    "            #model_param.states.append(np.reshape(state_t, [1,model_param.state_size]))\n",
    "            #model_param.states_next.append(np.reshape(state_next_t, [1,model_param.state_size]))\n",
    "            model_param.action_matrixs.append(np.reshape(action_matrix, [1,model_param.action_size]))\n",
    "            model_param.dones.append(np.reshape(0 if done else 1, [1,model_param.value_size]))\n",
    "            model_param.action_probs.append(np.reshape(action_prob, [1,model_param.action_size]))\n",
    "            model_param.rewards.append(np.reshape(reward, [1,model_param.value_size]))\n",
    "            \n",
    "            if(count % model_param.mini_batch_step_size == 0):\n",
    "                train_mini_batch(model_param)\n",
    "                clear_memory(model_param)\n",
    "                initialize()\n",
    "\n",
    "            reward_tot += reward\n",
    "            \n",
    "        return count, reward_tot\n",
    "\n",
    "def make_gae(values, values_next, rewards, dones, discount_rate, smooth_rate):\n",
    "    delta_adv, delta_tar, adv, target = 0, 0, 0, 0\n",
    "    advantages = np.zeros(np.array(values).shape)\n",
    "    targets = np.zeros(np.array(values).shape)\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        delta_adv = rewards[t] + discount_rate * values_next[t] * dones[t] - values[t]\n",
    "        delta_tar = rewards[t] + discount_rate * values_next[t] * dones[t]\n",
    "        adv = delta_adv + smooth_rate * discount_rate * dones[t] * adv\n",
    "        target = delta_tar + smooth_rate * discount_rate * dones[t] * target\n",
    "        advantages[t] = adv\n",
    "        targets[t] = target\n",
    "    return advantages, targets\n",
    "\n",
    "\n",
    "def train_mini_batch(model_param):\n",
    "    if len(model_param.rewards) == 0:\n",
    "        return\n",
    "\n",
    "    # Convert collected data to numpy arrays\n",
    "    action_matrixs_t = np.array(model_param.action_matrixs)\n",
    "    action_probs_t = np.array(model_param.action_probs)\n",
    "    rewards_t = np.array(model_param.rewards)\n",
    "    dones_t = np.array(model_param.dones)\n",
    "\n",
    "    # Use image states directly\n",
    "    states_t = np.vstack(model_param.states)       # (batch_size, H, W, C)\n",
    "    states_next_t = np.vstack(model_param.states_next)\n",
    "\n",
    "    # Predict values from critic\n",
    "    values = model_param.model_critic.predict(states_t)\n",
    "    values_next = model_param.model_critic.predict(states_next_t)\n",
    "\n",
    "    # Calculate advantages and targets using GAE\n",
    "    advantages_t, targets_t = make_gae(values, values_next, rewards_t, dones_t,\n",
    "                                       model_param.discount_rate, model_param.smooth_rate)\n",
    "\n",
    "    # Train actor and critic\n",
    "    states_t = np.array(states_t, dtype=np.float32)\n",
    "    action_matrixs_t = np.array(action_matrixs_t, dtype=np.float32)\n",
    "    advantages_t = np.array(advantages_t, dtype=np.float32)\n",
    "    action_probs_t = np.array(action_probs_t, dtype=np.float32)\n",
    "\n",
    "    # 입력 차원 수정\n",
    "    #action_matrixs_t = np.expand_dims(action_matrixs_t, axis=1)  # (batch_size, 1, action_size)\n",
    "    #advantages_t = np.expand_dims(advantages_t, axis=1)          # (batch_size, 1, value_size)\n",
    "\n",
    "    # PPO Actor 학습\n",
    "    model_param.model_actor.fit([states_t, action_matrixs_t, advantages_t], [action_probs_t],\n",
    "                                epochs=model_param.epochs_cnt, verbose=0)\n",
    "    model_param.model_critic.fit(states_t, targets_t, epochs=model_param.epochs_cnt, verbose=0)\n",
    "\n",
    "\n",
    "def clear_memory(model_param):\n",
    "    model_param.states, model_param.states_next, model_param.action_matrixs = [], [], []\n",
    "    model_param.dones, model_param.action_probs, model_param.rewards = [], [], []\n",
    "\n",
    "def moving_avg(data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "def save_model(model_param):\n",
    "    model_param.model_actor.save(\"./model/ppo_actor\")\n",
    "    model_param.model_critic.save(\"./model/ppo_critic\")\n",
    "    print(\"***** Training Complete and Model Saved *****\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'model_param' in locals():\n",
    "        del model_param\n",
    "\n",
    "    model_param = AI_init()\n",
    "    train(model_param)\n",
    "    save_model(model_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_unit = np.zeros(model_param.action_size) # 12\n",
    "action_unit= [1,1,1,1,1,1,1,1,1,1,1,1] \n",
    "\n",
    "initialize()\n",
    "base_pose = np.array([0, 0, 0.5])\n",
    "action_pose = tool.get_tool_pose()[0] \n",
    "for _ in range(24 * 2):\n",
    "    tool.set_tool_pose(action_pose, p.getQuaternionFromEuler(base_orientation))\n",
    "    p.stepSimulation()\n",
    "\n",
    "camera_orientation = p.getQuaternionFromEuler([1.57, 0, 1.57])\n",
    "camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0])\n",
    "update_camera(camera_position, camera_orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50199083, 0.48733655, 0.51342403, 0.49689441])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tool.get_tool_pose()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_model_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │ input_states[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m24\u001b[0m)     │        \u001b[38;5;34m120\u001b[0m │ input_states[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │         \u001b[38;5;34m50\u001b[0m │ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> (680.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m170\u001b[0m (680.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> (680.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m170\u001b[0m (680.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m24\u001b[0m)          │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │            \u001b[38;5;34m25\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> (580.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m145\u001b[0m (580.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> (580.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m145\u001b[0m (580.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 203\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    202\u001b[0m     agent \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[1;32m--> 203\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 93\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m state \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmax_episode_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m---> 93\u001b[0m count, reward_tot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_mini_batch()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_memory()\n",
      "Cell \u001b[1;32mIn[18], line 129\u001b[0m, in \u001b[0;36mAgent.make_memory\u001b[1;34m(self, episode, state)\u001b[0m\n\u001b[0;32m    126\u001b[0m action_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size) \u001b[38;5;66;03m#초기화\u001b[39;00m\n\u001b[0;32m    127\u001b[0m action_matrix[action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 129\u001b[0m state_next, reward, done, none, none2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m state_next_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state_next,[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_size])\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[0;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\__init__.py:424\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.client import device_lib\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "LOSS_CLIPPING = 0.2\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.node_num = 24\n",
    "        self.learning_rate_actor = 0.0005\n",
    "        self.learning_rate_critic = 0.0005\n",
    "        self.epochs_cnt = 5\n",
    "        self.model_actor = self.build_model_actor()\n",
    "        self.model_critic = self.build_model_critic()\n",
    "        \n",
    "        self.discount_rate = 0.98\n",
    "        self.smooth_rate = 0.95\n",
    "        self.penalty = -400\n",
    "        \n",
    "        self.episode_num = 500\n",
    "        self.mini_batch_step_size = 32        \n",
    "        \n",
    "        self.moving_avg_size = 20\n",
    "        self.reward_list= []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        \n",
    "        self.states, self.states_next, self.action_matrixs = [],[],[]\n",
    "        self.dones, self.action_probs, self.rewards = [],[],[]\n",
    "        self.DUMMY_ACTION_MATRIX = np.zeros((1,1,self.action_size))\n",
    "        self.DUMMY_ADVANTAGE = np.zeros((1,1,self.value_size))\n",
    "        \n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages = in_datas[0], in_datas[1], in_datas[2]\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)\n",
    "                new_policy = K.max(action_matrixs*y_pred, axis=-1)   \n",
    "                old_policy = K.max(action_matrixs*out_action_probs, axis=-1)   \n",
    "                r = new_policy/(old_policy)\n",
    "                clipped = K.clip(r, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)\n",
    "                loss = -K.minimum(r*advantages, clipped*advantages)\n",
    "            \n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "    def build_model_actor(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(1,self.action_size), name='input_action_matrixs')\n",
    "        input_advantages = Input(shape=(1,self.value_size), name='input_advantages')\n",
    "\n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='relu')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "        \n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_advantages], outputs=out_actions)\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate_actor))\n",
    "        \n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def build_model_critic(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='relu')(x)\n",
    "        out_values = Dense(self.value_size, activation='linear', name='output')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate_critic),\n",
    "                      loss='mean_squared_error'\n",
    "                     )\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            self.env.max_episode_steps = 500\n",
    "\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            self.train_mini_batch()\n",
    "            self.clear_memory()\n",
    "            \n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot-self.penalty\n",
    "                \n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))                \n",
    "            \n",
    "            if(episode % 10 == 0):\n",
    "                print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, self.moving_avg_list[-1], np.mean(self.reward_list)))\n",
    "        self.save_model()\n",
    "        \n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        advantage = np.zeros(self.value_size)\n",
    "        target = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            state_t = np.reshape(state,[1, 1, self.state_size])\n",
    "            action_matrix_t = np.reshape(action_matrix,[1, 1, self.action_size])\n",
    "            \n",
    "            action_prob = self.model_actor.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE])\n",
    "\n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
    "            action_matrix = np.zeros(self.action_size) #초기화\n",
    "            action_matrix[action] = 1\n",
    "\n",
    "            state_next, reward, done, none, none2 = self.env.step(action)\n",
    "            \n",
    "            state_next_t = np.reshape(state_next,[1, 1, self.state_size])\n",
    "            \n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty \n",
    "        \n",
    "            self.states.append(np.reshape(state_t, [1,self.state_size]))\n",
    "            self.states_next.append(np.reshape(state_next_t, [1,self.state_size]))\n",
    "            self.action_matrixs.append(np.reshape(action_matrix, [1,self.action_size]))\n",
    "            self.dones.append(np.reshape(0 if done else 1, [1,self.value_size]))\n",
    "            self.action_probs.append(np.reshape(action_prob, [1,self.action_size]))\n",
    "            self.rewards.append(np.reshape(reward, [1,self.value_size]))\n",
    "            \n",
    "            if(count % self.mini_batch_step_size == 0):\n",
    "                self.train_mini_batch()\n",
    "                self.clear_memory()\n",
    "\n",
    "            reward_tot += reward\n",
    "            state = state_next\n",
    "            \n",
    "        return count, reward_tot\n",
    "        \n",
    "    def make_gae(self, values, values_next, rewards, dones):\n",
    "        delta_adv, delta_tar, adv, target = 0, 0, 0, 0\n",
    "        advantages = np.zeros(np.array(values).shape)\n",
    "        targets = np.zeros(np.array(values).shape)\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            delta_adv = rewards[t] + self.discount_rate * values_next[t] * dones[t] - values[t]\n",
    "            delta_tar = rewards[t] + self.discount_rate * values_next[t] * dones[t]\n",
    "            adv = delta_adv + self.smooth_rate*self.discount_rate * dones[t] * adv\n",
    "            target = delta_tar + self.smooth_rate*self.discount_rate * dones[t] * target\n",
    "            advantages[t] = adv\n",
    "            targets[t] = target\n",
    "        return advantages, targets\n",
    "\n",
    "    def train_mini_batch(self):\n",
    "        \n",
    "        if len(self.states) == 0:\n",
    "            return\n",
    "        \n",
    "        states_t = np.array(self.states)\n",
    "        states_next_t = np.array(self.states_next)\n",
    "        action_matrixs_t = np.array(self.action_matrixs)\n",
    "        action_probs_t = np.array(self.action_probs)\n",
    "        rewards_t = np.array(self.rewards)\n",
    "\n",
    "        values = self.model_critic.predict(states_t)\n",
    "        values_next = self.model_critic.predict(states_next_t)\n",
    "        \n",
    "        advantages, targets = self.make_gae(values, values_next, self.rewards, self.dones)\n",
    "        advantages_t = np.array(advantages)\n",
    "        targets_t = np.array(targets)\n",
    "\n",
    "        self.model_actor.fit([states_t, action_matrixs_t, advantages_t], [action_probs_t], epochs=self.epochs_cnt, verbose=0)\n",
    "        self.model_critic.fit(states_t, targets_t, epochs=self.epochs_cnt, verbose=0)       \n",
    " \n",
    "    def clear_memory(self):\n",
    "        self.states, self.states_next, self.action_matrixs = [],[],[]\n",
    "        self.dones, self.action_probs, self.rewards = [],[],[]\n",
    "        \n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model_actor.save(\"./model/ppo\")\n",
    "        print(\"*****end learing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.observation_space.shape[0] # 위치, 속도, 각도, 각속도?\n",
    "env.action_space.n # 좌우 2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79932191 0.03632879 1.39198408]\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "action_unit = [0.01,-0.01,0.01,-0.01,0.01,-0.01] # +- 순서로 액션 을 정함\n",
    "\n",
    "action_pose = np.array(tool.get_tool_pose()[0])\n",
    "if action in [0, 1]:\n",
    "    # x축 변경\n",
    "\n",
    "    action_pose[0] += action_unit[action]\n",
    "elif action in [2, 3]:\n",
    "    # y축 변경\n",
    "    action_pose[1] += action_unit[action]\n",
    "elif action in [4, 5]:\n",
    "    # z축 변경\n",
    "    action_pose[2] += action_unit[action]\n",
    "\n",
    "print(action_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos diff:  0.16357298126706452  velocity:  0.0  angle diff:  0.02465193761612129  angular velocity:  0.0\n",
      "angle:  [-0.02178325 -0.01130558 -0.00232204] angle_specimen [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "if 'state' not in locals():\n",
    "    state =np.zeros(4)\n",
    "\n",
    "state = get_state(state)\n",
    "\n",
    "\n",
    "# if variable state does not exsit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
