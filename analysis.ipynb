{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pybullet_data\n"
     ]
    }
   ],
   "source": [
    "import pybullet_data\n",
    "import pybullet as p\n",
    "import pybullet_industrial as pi\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "p.connect(p.GUI)  # Use p.DIRECT for headless\n",
    "#p.connect(p.DIRECT)\n",
    "data_path = pybullet_data.getDataPath()\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "print(pybullet_data.getDataPath())\n",
    "plane_id = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Get the path to the PyBullet data directory\n",
    "data_path = pybullet_data.getDataPath()\n",
    "\n",
    "# List the files in the PyBullet data directory\n",
    "files = os.getcwd()\n",
    "dirname = os.path.join(files,'robots')\n",
    "assets = os.path.join(files,'assets')\n",
    "\n",
    "urdf_file1 = os.path.join(dirname,'rb5_850e.urdf')\n",
    "asset1 = os.path.join(assets,'specimen.urdf')\n",
    "#endeffector = os.path.join(assets,'endeffector.urdf')\n",
    "endeffector = os.path.join(assets,\n",
    "                                'gripper_cylinder.urdf')\n",
    "\n",
    "start_orientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "\n",
    "cube1 = p.loadURDF(\"cube.urdf\", np.array(\n",
    "        [0, 0, 0.5]), useFixedBase=True)\n",
    "robot = pi.RobotBase(urdf_file1, np.array(\n",
    "        [0, 0, 1]), start_orientation)\n",
    "cube2 = p.loadURDF(\"cube.urdf\", np.array(\n",
    "        [1, 0, 0.5]), useFixedBase=True)\n",
    "specimen = p.loadURDF(asset1, np.array(\n",
    "[1.0, 0, 1.3]), useFixedBase=True)\n",
    "meshScale = [1, 1, 1]\n",
    "\n",
    "pi.draw_robot_frames(robot, life_time=0)\n",
    "\n",
    "tool = pi.EndeffectorTool(endeffector, np.array([2.0, 0, 1.5]), start_orientation, robot)\n",
    "p.setGravity(0, 0, -9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    position_list = [1,0.16175214925291828,0.49468914613294146,1.4254185043970449,-0.350008612250527,1.5717769960272012,-0.1634941307782464,0]\n",
    "    gripper_position = [0.5, 0, 1.1]\n",
    "    gripper_orientation = [1, 1, 1, 1]\n",
    "    #update_gripper(gripper_position, gripper_orientation)\n",
    "\n",
    "    pose_dict = {\n",
    "    \"base\": 0.16175214925291828,\n",
    "    \"shoulder\": 0.49468914613294146,\n",
    "    \"elbow\": 1.4254185043970449,\n",
    "    \"wrist1\": -0.3500086122505276,\n",
    "    \"wrist2\": 1.5717769960272012,\n",
    "    \"wrist3\": -0.1634941307782464\n",
    "\n",
    "    }\n",
    "\n",
    "    pi.RobotBase.reset_robot(robot,[0, 0, 1], start_orientation, position_list)\n",
    "    pi.RobotBase.set_joint_position(robot,pose_dict)\n",
    "    for i in range(1000):\n",
    "      p.stepSimulation()\n",
    "\n",
    "def check_collision(visualize=False):\n",
    "    contacts = p.getContactPoints() \n",
    "    contacts = [c for c in contacts if c[1] >= 4] # 로봇의 베이스나, 엔드이펙터와 플랜지사이의 충돌은 무시\n",
    "    if contacts:\n",
    "        print(\"TCP 링크가 무언가와 충돌 발생!\")\n",
    "        return True\n",
    "        for contact in contacts:\n",
    "            print(contact)\n",
    "    \n",
    "    if visualize:\n",
    "        for c in contacts:\n",
    "            pos_onB = c[6]\n",
    "            p.removeAllUserDebugItems()\n",
    "            p.addUserDebugPoints(\n",
    "                pointPositions=[pos_onB],\n",
    "                pointColorsRGB=[[1, 0, 0]],   # 빨간색\n",
    "                pointSize=50,\n",
    "                lifeTime=10\n",
    "            )\n",
    "\n",
    "def update_camera(camera_position, camera_orientation):\n",
    "    camera_id = None\n",
    "    for body_id in range(p.getNumBodies()):\n",
    "        body_name = p.getBodyInfo(body_id)[1].decode()\n",
    "        if \"camera\" in body_name:  # 카메라 URDF 파일명이 포함된 경우\n",
    "            camera_id = body_id\n",
    "            break\n",
    "    pos_offset = np.array([0.1, 0, 0.04])\n",
    "    ori_offset = np.array([0.5, 0.5, 0, 0])\n",
    "    p.resetBasePositionAndOrientation(camera_id, camera_position + pos_offset, camera_orientation + ori_offset)\n",
    "\n",
    "def update_specimen(specimen_position, specimen_orientation):\n",
    "    specimen_id = None\n",
    "    for body_id in range(p.getNumBodies()):\n",
    "        body_name = p.getBodyInfo(body_id)[1].decode()\n",
    "        if \"my_specimen\" in body_name:  # my_specimen URDF 파일명이 포함된 경우\n",
    "            specimen_id = body_id\n",
    "            break\n",
    "    p.resetBasePositionAndOrientation(specimen_id, specimen_position, specimen_orientation)\n",
    "\n",
    "def update_gripper(gripper_position, gripper_orientation):\n",
    "    gripper_id = None\n",
    "    for body_id in range(p.getNumBodies()):\n",
    "        body_name = p.getBodyInfo(body_id)[1].decode()\n",
    "        if \"gripper\" in body_name:  # gripper URDF 파일명이 포함된 경우\n",
    "            gripper_id = body_id\n",
    "            break\n",
    "    p.resetBasePositionAndOrientation(gripper_id, gripper_position, gripper_orientation)\n",
    "\n",
    "def show_point(tool_pos):\n",
    "    p.addUserDebugPoints(\n",
    "    pointPositions=[tool_pos],\n",
    "    pointColorsRGB=[[1, 0, 1]],  \n",
    "    pointSize=10,\n",
    "    lifeTime=1\n",
    ")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize()\n",
    "\n",
    "urdf_file3 = os.path.join(assets, 'camera.urdf')\n",
    "camera_parameters = {'width': 480, 'height': 240, 'fov': 60,\n",
    "                        'aspect ratio': 1, 'near plane distance': 0.01, 'far plane distance': 2}\n",
    "camera_orientation = p.getQuaternionFromEuler([1.57, 0, 1.57])\n",
    "camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0.1])\n",
    "camera = pi.Camera(urdf_file3, camera_position,\n",
    "                camera_orientation, camera_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pose = np.array([0, 0, 0.5])\n",
    "set_pose = np.array([0.860, 0, 0.81])\n",
    "base_orientation = np.array([1.57, 0, 1.57])\n",
    "\n",
    "target_pose = base_pose + set_pose\n",
    "\n",
    "\n",
    "specimen_position = np.array([1.0, 0, 1.30])\n",
    "ring_offset = np.array([0, 0, 0.015])\n",
    "specimen_orientation_euler = np.array([0, 0, 0])\n",
    "specimen_orientation = p.getQuaternionFromEuler(specimen_orientation_euler)\n",
    "update_specimen(specimen_position, specimen_orientation)\n",
    "\n",
    "for _ in range(24 * 1):  # 240 Hz simulation frequency\n",
    "    tool.set_tool_pose(target_pose, p.getQuaternionFromEuler(base_orientation))\n",
    "    img = camera.get_image()\n",
    "    p.stepSimulation()\n",
    "    camera_orientation = p.getQuaternionFromEuler([1.57, 0, 1.57])\n",
    "    camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0])\n",
    "    update_camera(camera_position, camera_orientation)\n",
    "    show_point(tool.get_tool_pose()[0]+np.array([0.135,0,0]))\n",
    "    show_point(specimen_position+ring_offset)\n",
    "    #time.sleep(1./24.)\n",
    "\n",
    "check_collision(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_model_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │ input_states[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │ conv2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93184</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23,855,360</span> │ flatten_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,542</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m480\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_60 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m119\u001b[0m,   │      \u001b[38;5;34m6,176\u001b[0m │ input_states[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_61 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m58\u001b[0m,    │     \u001b[38;5;34m32,832\u001b[0m │ conv2d_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_62 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93184\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv2d_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │ \u001b[38;5;34m23,855,360\u001b[0m │ flatten_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │      \u001b[38;5;34m1,542\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,932,838</span> (91.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,932,838\u001b[0m (91.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,932,838</span> (91.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,932,838\u001b[0m (91.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">119</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93184</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,855,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m, \u001b[38;5;34m480\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_63 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m119\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_64 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_65 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_21 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93184\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m23,855,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,931,553</span> (91.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,931,553\u001b[0m (91.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,931,553</span> (91.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,931,553\u001b[0m (91.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "5\n",
      "[0.16999242 0.14014715 0.203756   0.14705141 0.14985655 0.18919645]\n",
      "reward:  13.164196269445595\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[0.17090751 0.13907647 0.20593429 0.15085693 0.15106735 0.18215738]\n",
      "reward:  12.965071926796329\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "1\n",
      "[0.17289625 0.13910413 0.20488928 0.1494199  0.15063538 0.18305507]\n",
      "reward:  12.111401467876131\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "0\n",
      "[0.17193045 0.13943088 0.2051626  0.14947519 0.15008071 0.18392019]\n",
      "reward:  12.888650765900652\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "3\n",
      "[0.16978765 0.13818467 0.20636483 0.15060273 0.15122981 0.18383035]\n",
      "reward:  12.84571056841688\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[0.1731803  0.13715015 0.20438157 0.14922726 0.14997786 0.18608287]\n",
      "reward:  12.541627480083829\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "4\n",
      "[0.1720516  0.14041066 0.206259   0.14887583 0.14963193 0.18277098]\n",
      "reward:  12.78903776030626\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "2\n",
      "[0.17134291 0.13812959 0.20499688 0.14897244 0.15113959 0.18541859]\n",
      "reward:  12.832399311238323\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.17348546 0.1388551  0.20321104 0.14967246 0.1511868  0.18358909]\n",
      "reward:  12.505162414892304\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "3\n",
      "[0.17267692 0.13865167 0.20540103 0.15006195 0.14862785 0.1845806 ]\n",
      "reward:  12.409448628907855\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "4\n",
      "[0.17136966 0.13931946 0.20662928 0.14930597 0.14854766 0.18482794]\n",
      "reward:  12.687811371902963\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "1\n",
      "[0.17215909 0.13957131 0.20337681 0.149119   0.1506276  0.18514617]\n",
      "reward:  11.898426772371012\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "0\n",
      "[0.17154807 0.13974147 0.20655271 0.14954993 0.14907171 0.18353619]\n",
      "reward:  12.612854497632437\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "0\n",
      "[0.17211464 0.13958648 0.20450076 0.14941694 0.15032183 0.18405935]\n",
      "reward:  13.453452052849093\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "0\n",
      "[0.1706883  0.1404541  0.20522809 0.14914665 0.15048017 0.18400273]\n",
      "reward:  14.378085132715292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "0\n",
      "[0.17060597 0.14016198 0.20409508 0.15040483 0.15015568 0.18457644]\n",
      "reward:  15.394122071776998\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "0\n",
      "[0.17338963 0.14009237 0.20166333 0.15046053 0.14963372 0.18476042]\n",
      "reward:  16.502614141984807\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "2\n",
      "[0.17305951 0.13994631 0.20117785 0.14990515 0.14999774 0.18591349]\n",
      "reward:  16.604570542020454\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "0\n",
      "[0.17411841 0.1402559  0.20156142 0.15174209 0.14984128 0.18248093]\n",
      "reward:  17.77029764570137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "0\n",
      "[0.17433538 0.13615656 0.20261972 0.15283117 0.14907941 0.18497775]\n",
      "reward:  19.046228834781456\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "1\n",
      "[0.17407937 0.13605277 0.20106433 0.15314572 0.14858538 0.18707253]\n",
      "reward:  17.63286241644614\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "1\n",
      "[0.17494844 0.13756971 0.2029487  0.1519021  0.1490109  0.18362013]\n",
      "reward:  16.28978286236027\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[0.17315567 0.13732743 0.20556936 0.15109247 0.14906496 0.18379013]\n",
      "reward:  15.31552092323957\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "2\n",
      "[0.17169185 0.1401416  0.20451806 0.15115878 0.14998414 0.1825055 ]\n",
      "reward:  15.195440410876628\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.17077348 0.14313075 0.20424844 0.14787099 0.15327676 0.18069954]\n",
      "reward:  14.969633988769237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.17219299 0.14458811 0.2004446  0.14578135 0.1517093  0.18528356]\n",
      "reward:  14.62941952215664\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[0.17081994 0.14629655 0.19856462 0.14645599 0.15348771 0.18437517]\n",
      "reward:  14.79489710412899\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "0\n",
      "[0.17201233 0.14181122 0.20562075 0.14583968 0.15110947 0.18360652]\n",
      "reward:  15.550071703525566\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "0\n",
      "[0.17301944 0.14333391 0.20234032 0.14647411 0.15055148 0.18428078]\n",
      "reward:  16.314481684379547\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "4\n",
      "[0.17462607 0.14268027 0.20041111 0.14580214 0.15075816 0.1857222 ]\n",
      "reward:  17.428853130413135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "2\n",
      "[0.17382245 0.14019355 0.20269445 0.146503   0.14897001 0.18781655]\n",
      "reward:  16.953982444181626\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "5\n",
      "[0.17199126 0.14376834 0.19830725 0.14617059 0.15221451 0.1875481 ]\n",
      "reward:  15.697475090122502\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "0\n",
      "[0.1713401  0.14570291 0.19789769 0.1447605  0.15068991 0.18960893]\n",
      "reward:  16.26107415653194\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.17175192 0.14594184 0.19625774 0.14600427 0.15062363 0.18942058]\n",
      "reward:  15.70442122933524\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "1\n",
      "[0.17299275 0.14294234 0.19857432 0.14911753 0.14961293 0.18676007]\n",
      "reward:  14.956798548239712\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[0.17156689 0.14520858 0.19936508 0.14845833 0.15000112 0.1854    ]\n",
      "reward:  13.895788572114437\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.17333128 0.14430958 0.19717926 0.14771497 0.14940117 0.18806374]\n",
      "reward:  12.924638583337913\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "1\n",
      "[0.17135991 0.14578934 0.19923651 0.14688392 0.15072542 0.18600497]\n",
      "reward:  12.419216609908881\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "4\n",
      "[0.17119862 0.14479771 0.20076416 0.14742509 0.1489175  0.18689694]\n",
      "reward:  13.076996005390672\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "2\n",
      "[0.17350769 0.14547996 0.1968993  0.14740515 0.15006351 0.18664436]\n",
      "reward:  12.705271564041134\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "0\n",
      "[0.17137247 0.14160258 0.1993749  0.1495811  0.15231475 0.18575425]\n",
      "reward:  13.044761096328294\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "0\n",
      "[0.17176722 0.14078313 0.19751188 0.14943075 0.15642858 0.18407847]\n",
      "reward:  13.362207256832422\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "3\n",
      "[0.17115195 0.14112233 0.20006025 0.14707693 0.1541346  0.18645394]\n",
      "reward:  13.642029822613091\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "0\n",
      "[0.17021896 0.14559448 0.20195822 0.14612235 0.14995836 0.18614766]\n",
      "reward:  13.90346556612558\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "4\n",
      "[0.16935426 0.14630784 0.20143154 0.14644913 0.14937618 0.18708102]\n",
      "reward:  14.872693764842046\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "0\n",
      "[0.17244719 0.14404279 0.19827403 0.14831331 0.14918879 0.18773383]\n",
      "reward:  15.143415493271307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[0.16965526 0.1453568  0.19699678 0.14993106 0.14923823 0.18882194]\n",
      "reward:  13.900359304470332\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "2\n",
      "[0.16811307 0.14463574 0.20001894 0.14703752 0.1522868  0.18790795]\n",
      "reward:  13.406315233699695\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "2\n",
      "[0.16840172 0.14112042 0.1987999  0.15044829 0.15403792 0.18719174]\n",
      "reward:  12.89348101141843\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "2\n",
      "[0.17006688 0.14090273 0.19951247 0.15107977 0.15226234 0.18617576]\n",
      "reward:  12.361948602091404\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[0.23648742 0.11366039 0.21872793 0.11081506 0.09855101 0.22175819]\n",
      "reward:  13.184227255877977\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "0\n",
      "[0.23650075 0.11528748 0.21898386 0.11120299 0.09971441 0.21831045]\n",
      "reward:  14.17459567865386\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "0\n",
      "[0.23743613 0.11483936 0.21717995 0.11104564 0.09897038 0.22052859]\n",
      "reward:  15.352608934215969\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "5\n",
      "[0.2371847  0.11528736 0.21714029 0.11138646 0.0993743  0.21962686]\n",
      "reward:  15.045509405499418\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "3\n",
      "[0.23592837 0.11552604 0.2190353  0.11188703 0.09942452 0.21819875]\n",
      "reward:  14.957457486896374\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "4\n",
      "[0.23781353 0.11438997 0.2165551  0.11176068 0.0987628  0.22071786]\n",
      "reward:  15.297598020406793\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "2\n",
      "[0.23652999 0.11415608 0.21683075 0.11192675 0.09911016 0.2214463 ]\n",
      "reward:  15.354296291854132\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "1\n",
      "[0.23769633 0.11467998 0.21663275 0.11140937 0.09882309 0.2207585 ]\n",
      "reward:  14.207927834013972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "2\n",
      "[0.2371059  0.1154182  0.21792823 0.11148211 0.09966213 0.21840344]\n",
      "reward:  14.098632297717417\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "3\n",
      "[0.2380354  0.11458696 0.21721154 0.11168233 0.09886529 0.21961848]\n",
      "reward:  14.159804640257056\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "2\n",
      "[0.23722765 0.11464282 0.2177194  0.1119644  0.09933204 0.21911366]\n",
      "reward:  14.078210061149452\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.23809406 0.11459214 0.2165181  0.11201621 0.09925476 0.21952482]\n",
      "reward:  14.137298228067403\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "2\n",
      "[0.23742326 0.11448158 0.21796939 0.11184224 0.09925994 0.2190236 ]\n",
      "reward:  14.050728871814696\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "4\n",
      "[0.23898177 0.11431707 0.21618874 0.11182326 0.09897815 0.21971098]\n",
      "reward:  14.307465535004283\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.23904568 0.11419901 0.21583082 0.1115986  0.0984875  0.22083847]\n",
      "reward:  14.394859968287546\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "2\n",
      "[0.23743951 0.11468843 0.21714823 0.11187748 0.09898714 0.21985918]\n",
      "reward:  14.306922683957637\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "0\n",
      "[0.23829691 0.11447845 0.2170596  0.11187173 0.09883921 0.2194541 ]\n",
      "reward:  15.421945070728919\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.23771174 0.11421743 0.21595535 0.11168376 0.09907163 0.22136003]\n",
      "reward:  15.033588186253146\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "4\n",
      "[0.23738356 0.11505014 0.2165426  0.11196191 0.09920681 0.21985498]\n",
      "reward:  15.40885398728324\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "1\n",
      "[0.2376058  0.11446247 0.21657579 0.1115961  0.09884148 0.22091827]\n",
      "reward:  14.282703433678993\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "0\n",
      "[0.23755422 0.11478475 0.21723819 0.11167163 0.09937035 0.21938083]\n",
      "reward:  15.3346464940308\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.23773757 0.11413515 0.216055   0.11137493 0.09872815 0.22196914]\n",
      "reward:  14.892388963022833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "4\n",
      "[0.237513   0.11504747 0.21741253 0.11177753 0.09915934 0.21909015]\n",
      "reward:  15.307454477252197\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "1\n",
      "[0.23847722 0.11426359 0.21597761 0.11123236 0.09879632 0.22125281]\n",
      "reward:  14.207963024959714\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "5\n",
      "[0.23727086 0.11475369 0.21727136 0.11159786 0.09928167 0.21982455]\n",
      "reward:  13.767744651525437\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.23725726 0.11516479 0.21720202 0.11153368 0.0990828  0.2197595 ]\n",
      "reward:  13.271442566894933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "3\n",
      "[0.23639646 0.11497863 0.21710579 0.11174928 0.0999063  0.21986347]\n",
      "reward:  13.297859155951596\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "2\n",
      "[0.23603812 0.11543877 0.2176735  0.11200546 0.09947912 0.219365  ]\n",
      "reward:  13.184125053382202\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "2\n",
      "[0.23643431 0.11488574 0.21763782 0.11136133 0.10016745 0.2195133 ]\n",
      "reward:  12.978953130090607\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "0\n",
      "[0.23635396 0.11515659 0.21646614 0.11172184 0.10015449 0.22014692]\n",
      "reward:  13.707228342930806\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "2\n",
      "[0.23721944 0.11554871 0.2163091  0.11114518 0.09978653 0.21999104]\n",
      "reward:  13.403240489109665\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.23614287 0.11562486 0.21717033 0.11140173 0.10018391 0.21947627]\n",
      "reward:  13.62298005370196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.23669034 0.11561908 0.21690722 0.11127508 0.09955981 0.2199484 ]\n",
      "reward:  12.981835702949583\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "0\n",
      "[0.23494005 0.11585532 0.21689285 0.11247544 0.10044306 0.2193933 ]\n",
      "reward:  13.579485446621891\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "3\n",
      "[0.23699541 0.1154754  0.21662784 0.11112312 0.09941401 0.22036429]\n",
      "reward:  13.726711845527731\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.23585941 0.11488972 0.2173404  0.11165742 0.09988163 0.2203714 ]\n",
      "reward:  12.937215881961066\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.23609792 0.11502498 0.2160817  0.11162713 0.10051306 0.22065522]\n",
      "reward:  12.16496118718212\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "2\n",
      "[0.23557103 0.11517399 0.21626574 0.11160841 0.10092031 0.22046055]\n",
      "reward:  11.949619579374994\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.23440729 0.11558738 0.2168441  0.11318983 0.10083815 0.21913323]\n",
      "reward:  11.267628413454654\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "2\n",
      "[0.23537076 0.11554925 0.21708211 0.11243265 0.10045619 0.21910903]\n",
      "reward:  11.032850577283776\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "3\n",
      "[0.23608395 0.11503626 0.21627627 0.11244167 0.10119689 0.21896496]\n",
      "reward:  11.128643127035458\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "2\n",
      "[0.23546124 0.11580415 0.2174901  0.11201974 0.10031109 0.2189136 ]\n",
      "reward:  10.917408969414284\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "4\n",
      "[0.2351921  0.11515975 0.21662657 0.11243027 0.10142688 0.21916448]\n",
      "reward:  11.42435956619139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "5\n",
      "[0.23476537 0.11549222 0.21611573 0.11254029 0.10147429 0.21961214]\n",
      "reward:  10.809086049827567\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "3\n",
      "[0.23490906 0.1154854  0.21670677 0.11208105 0.10150523 0.21931247]\n",
      "reward:  10.88624784384055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "0\n",
      "[0.23559435 0.11561434 0.21755742 0.11218426 0.10022842 0.21882127]\n",
      "reward:  11.149400693121898\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.23500563 0.11584469 0.21716636 0.1128524  0.10064948 0.2184814 ]\n",
      "reward:  10.501540393682424\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "0\n",
      "[0.23521742 0.1155562  0.21717928 0.11262997 0.10050567 0.21891141]\n",
      "reward:  10.665326985558481\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "2\n",
      "[0.23519859 0.11558212 0.217215   0.11238983 0.10084762 0.21876684]\n",
      "reward:  10.468806675610331\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "0\n",
      "[0.23593475 0.11461414 0.21616249 0.1123333  0.10115516 0.21980017]\n",
      "reward:  10.60016098235652\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "1\n",
      "[0.14796452 0.11103843 0.23341672 0.13096866 0.10289152 0.27372018]\n",
      "reward:  12.369679861761592\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "2\n",
      "[0.14862281 0.11280521 0.23182985 0.13210753 0.10482428 0.26981032]\n",
      "reward:  12.312837104209523\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "0\n",
      "[0.1494013  0.11257143 0.23118827 0.13246852 0.10438406 0.26998648]\n",
      "reward:  13.199115428970952\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "4\n",
      "[0.14890197 0.11204625 0.23192102 0.13197158 0.10375423 0.27140492]\n",
      "reward:  13.275093045289728\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "3\n",
      "[0.14960864 0.11256898 0.23057611 0.13224845 0.10445363 0.2705441 ]\n",
      "reward:  13.355547984366913\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.14847802 0.11210694 0.23182912 0.13219064 0.10406545 0.2713298 ]\n",
      "reward:  13.313458422863516\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "1\n",
      "[0.14878595 0.11285426 0.23167987 0.13199352 0.10445233 0.27023402]\n",
      "reward:  12.414719382331485\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "4\n",
      "[0.14840928 0.11251514 0.23199764 0.13202222 0.10425424 0.27080148]\n",
      "reward:  12.455125303922019\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "4\n",
      "[0.14903341 0.11235885 0.23194775 0.13164851 0.10388974 0.27112177]\n",
      "reward:  12.469060027215662\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.14885607 0.11238368 0.2316155  0.132255   0.10428714 0.27060264]\n",
      "reward:  12.487072474816525\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.1489653  0.11228237 0.23175605 0.1321891  0.10423024 0.27057692]\n",
      "reward:  12.409249260448265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "0\n",
      "[0.14840318 0.11265641 0.23203002 0.13192004 0.10400631 0.27098408]\n",
      "reward:  13.286049726776302\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "3\n",
      "[0.1482118  0.11263724 0.23236327 0.13224873 0.10416453 0.27037445]\n",
      "reward:  13.303541221672441\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "4\n",
      "[0.14809206 0.11307344 0.23170938 0.13247262 0.10448533 0.2701672 ]\n",
      "reward:  13.443713231070673\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "0\n",
      "[0.1483166  0.11230373 0.23193236 0.13232587 0.10418142 0.27093998]\n",
      "reward:  14.503204064003365\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "0\n",
      "[0.14862798 0.112691   0.23157246 0.13241217 0.10439216 0.27030426]\n",
      "reward:  15.757123015496916\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.14865962 0.11293706 0.23132409 0.13257974 0.10485465 0.26964483]\n",
      "reward:  15.502049373349342\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.14812312 0.11261752 0.2321078  0.13220032 0.10402112 0.2709301 ]\n",
      "reward:  15.470795994729931\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "1\n",
      "[0.14834924 0.11256532 0.23262708 0.13173193 0.10396173 0.27076468]\n",
      "reward:  14.313824990338677\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.14874937 0.11283205 0.23175064 0.13260384 0.10459775 0.26946637]\n",
      "reward:  13.915653217883428\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "4\n",
      "[0.1487953  0.11256355 0.23178385 0.13215558 0.10405584 0.2706458 ]\n",
      "reward:  14.237589440099804\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "5\n",
      "[0.14860807 0.11245188 0.23225498 0.13238041 0.10421132 0.27009332]\n",
      "reward:  13.861272833504026\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "2\n",
      "[0.14889169 0.11271676 0.23135306 0.13216753 0.10417702 0.27069393]\n",
      "reward:  13.705040329298418\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "3\n",
      "[0.14844781 0.11265593 0.23178741 0.1321021  0.10409712 0.27090958]\n",
      "reward:  13.786270177831298\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.14843762 0.11226778 0.23194852 0.13204229 0.10420107 0.2711027 ]\n",
      "reward:  13.300125089636063\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "4\n",
      "[0.1491968  0.11246804 0.23059358 0.1325221  0.10451429 0.2707052 ]\n",
      "reward:  13.707384332419258\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.14883618 0.11263927 0.23179111 0.1321796  0.10436141 0.2701924 ]\n",
      "reward:  13.7109303344759\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "4\n",
      "[0.14856932 0.11273411 0.2317871  0.13215078 0.10409011 0.2706685 ]\n",
      "reward:  14.10550187230302\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "0\n",
      "[0.14848728 0.11281946 0.23182641 0.1323956  0.10438858 0.2700827 ]\n",
      "reward:  15.134737035941841\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "2\n",
      "[0.14825484 0.11249556 0.2327051  0.13184261 0.10408751 0.27061433]\n",
      "reward:  15.095312606153206\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.14898539 0.11258658 0.23161517 0.13264203 0.10419053 0.26998034]\n",
      "reward:  14.484890727354182\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "3\n",
      "[0.14864257 0.11249338 0.23204653 0.13219781 0.10409923 0.27052048]\n",
      "reward:  14.456996580527122\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[0.14820918 0.11317843 0.23239207 0.13195701 0.10432485 0.26993847]\n",
      "reward:  13.786582492218935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.14874244 0.11273564 0.23163886 0.13202983 0.10434602 0.27050716]\n",
      "reward:  13.076840772172506\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "2\n",
      "[0.14887178 0.1128356  0.23157398 0.13173805 0.10432094 0.27065966]\n",
      "reward:  12.967306847930038\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "1\n",
      "[0.14897041 0.11262938 0.2315347  0.13215294 0.10477612 0.26993638]\n",
      "reward:  12.265505003343348\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "1\n",
      "[0.14873745 0.11253083 0.23173432 0.13237508 0.10509326 0.2695291 ]\n",
      "reward:  11.599502915286314\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "4\n",
      "[0.14887568 0.11259728 0.23132682 0.13228495 0.10518549 0.26972982]\n",
      "reward:  11.989180708348421\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "2\n",
      "[0.14901336 0.11257748 0.23191878 0.13201723 0.10489412 0.26957905]\n",
      "reward:  11.866866591552307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.14889339 0.11270346 0.23095153 0.13220885 0.10506587 0.27017683]\n",
      "reward:  11.375830535957931\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "1\n",
      "[0.14866619 0.11254611 0.23072627 0.13209179 0.10488    0.2710896 ]\n",
      "reward:  10.801099955529425\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.14892328 0.11265619 0.23057304 0.13255978 0.10504814 0.2702396 ]\n",
      "reward:  10.360144564201304\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "1\n",
      "[0.1487021  0.11257038 0.23123728 0.13236135 0.10487094 0.27025795]\n",
      "reward:  9.88587691650491\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.14888331 0.11255302 0.23117264 0.13229468 0.10499065 0.27010566]\n",
      "reward:  9.50083719425518\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.14879212 0.1126551  0.23059244 0.13277581 0.10513247 0.270052  ]\n",
      "reward:  9.122895230600461\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "2\n",
      "[0.14870414 0.1127506  0.23103654 0.13239925 0.10505113 0.2700583 ]\n",
      "reward:  9.005949769559551\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "2\n",
      "[0.1489427  0.1129332  0.23098929 0.13248053 0.1053689  0.26928538]\n",
      "reward:  8.870656960716197\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "2\n",
      "[0.14892392 0.11288019 0.23137613 0.13212714 0.10502054 0.26967204]\n",
      "reward:  8.71331402401926\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "1\n",
      "[0.14858517 0.11284564 0.2307096  0.13290836 0.10523678 0.26971444]\n",
      "reward:  8.404146744703377\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "2\n",
      "[0.14883484 0.11265342 0.23079723 0.13257304 0.10490903 0.2702325 ]\n",
      "reward:  8.241428274574755\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.17651987 0.12165601 0.09375224 0.14783783 0.12367099 0.3365631 ]\n",
      "reward:  13.154554325681126\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "1\n",
      "[0.17732672 0.12320226 0.09565735 0.1487667  0.12498356 0.33006343]\n",
      "reward:  12.270652442066758\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "1\n",
      "[0.1768808  0.12319309 0.09548507 0.14858216 0.1250288  0.33083004]\n",
      "reward:  11.476790252920273\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.17700884 0.1232565  0.09541196 0.14876623 0.12517416 0.33038232]\n",
      "reward:  11.327462256314597\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "2\n",
      "[0.17707686 0.12309968 0.09572788 0.1485628  0.12447784 0.331055  ]\n",
      "reward:  11.273825086105399\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "1\n",
      "[0.17663811 0.12310079 0.09575506 0.14909942 0.12560022 0.3298064 ]\n",
      "reward:  10.620800346111183\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "1\n",
      "[0.17658262 0.12304    0.0955978  0.14907224 0.12533721 0.3303701 ]\n",
      "reward:  10.020602480681308\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "4\n",
      "[0.17663826 0.12302339 0.09590885 0.14907195 0.12549265 0.32986495]\n",
      "reward:  10.093261723429247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[0.17691891 0.12320702 0.09563212 0.14894868 0.12554033 0.32975292]\n",
      "reward:  10.126198978826253\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "4\n",
      "[0.1765937  0.12283128 0.09565353 0.14903629 0.12492035 0.33096486]\n",
      "reward:  10.191214182033669\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "4\n",
      "[0.17678225 0.12299956 0.09579178 0.1488238  0.12508921 0.3305134 ]\n",
      "reward:  10.227318655331924\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "1\n",
      "[0.17662065 0.12320367 0.09547719 0.14893396 0.12510513 0.3306594 ]\n",
      "reward:  9.677554110867483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "1\n",
      "[0.17668055 0.12294932 0.09561793 0.14887165 0.12490452 0.33097607]\n",
      "reward:  9.166658142926591\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "0\n",
      "[0.17670275 0.12309783 0.09588017 0.14895901 0.12505084 0.33030942]\n",
      "reward:  9.648104875881236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.17692322 0.12303342 0.09577198 0.14881626 0.12506868 0.33038637]\n",
      "reward:  9.626364869157316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.17678367 0.12301104 0.09568561 0.14897095 0.12513068 0.33041805]\n",
      "reward:  9.54528163515641\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "1\n",
      "[0.17677133 0.12318103 0.09589554 0.14926185 0.12534522 0.3295451 ]\n",
      "reward:  9.062049743876607\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "3\n",
      "[0.17684719 0.12308073 0.09588138 0.14920336 0.12525685 0.3297305 ]\n",
      "reward:  9.04649762383382\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "0\n",
      "[0.17668727 0.1236401  0.09597079 0.14933592 0.12571889 0.32864705]\n",
      "reward:  9.510542408590887\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "3\n",
      "[0.17686744 0.1233862  0.09603573 0.14888282 0.12533858 0.3294892 ]\n",
      "reward:  9.493280923907724\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "2\n",
      "[0.17656997 0.12355404 0.0960868  0.14919657 0.12562886 0.3289638 ]\n",
      "reward:  9.513174225005905\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[0.17705847 0.12348813 0.09594763 0.14907438 0.12555113 0.32888025]\n",
      "reward:  9.383328743517302\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "3\n",
      "[0.17700957 0.12337298 0.09593474 0.14922665 0.12555453 0.32890147]\n",
      "reward:  9.346026695460365\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[0.17666377 0.12361982 0.09589672 0.14923939 0.12528665 0.32929367]\n",
      "reward:  9.183949406708507\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "2\n",
      "[0.17661794 0.12370707 0.09604675 0.14922859 0.12573498 0.3286646 ]\n",
      "reward:  9.186279288252488\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "3\n",
      "[0.17712858 0.12352443 0.09573694 0.1494589  0.1256753  0.32847583]\n",
      "reward:  9.155286200703173\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.17679231 0.12366742 0.09598332 0.14912905 0.12584038 0.32858756]\n",
      "reward:  8.962822863502536\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "0\n",
      "[0.17670096 0.12357241 0.09589316 0.1492353  0.12592027 0.32867783]\n",
      "reward:  9.356539390744606\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[0.17654097 0.12355466 0.09599444 0.14926727 0.12584284 0.32879975]\n",
      "reward:  9.123800745741054\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.17680429 0.12357992 0.09578677 0.14915305 0.12595035 0.3287256 ]\n",
      "reward:  9.053469365657344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.17711443 0.12349292 0.09567608 0.14917918 0.12575473 0.32878262]\n",
      "reward:  9.075248961865213\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "0\n",
      "[0.1768203  0.12370711 0.09572279 0.14924687 0.12595783 0.32854506]\n",
      "reward:  9.4540912293328\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[0.17672454 0.12389699 0.09593796 0.14917031 0.12590446 0.32836568]\n",
      "reward:  9.164889035047098\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "3\n",
      "[0.17707923 0.12381205 0.09561403 0.14923333 0.12599505 0.3282663 ]\n",
      "reward:  9.092158600561163\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.17709652 0.12368368 0.09585668 0.1489926  0.12591277 0.3284577 ]\n",
      "reward:  8.788968022523528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "2\n",
      "[0.17747839 0.12378337 0.09553501 0.14914207 0.125661   0.32840014]\n",
      "reward:  8.789561865146108\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "2\n",
      "[0.17701954 0.1237238  0.09551409 0.14912924 0.12609036 0.32852298]\n",
      "reward:  8.770033031934608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[0.17671986 0.12390553 0.09577174 0.14928405 0.12578459 0.32853425]\n",
      "reward:  8.461919663866661\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "3\n",
      "[0.17706548 0.12380187 0.09577002 0.14923435 0.1255949  0.32853347]\n",
      "reward:  8.423789548966825\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "2\n",
      "[0.17733718 0.12353195 0.0953987  0.14917836 0.12595348 0.32860023]\n",
      "reward:  8.400518717103772\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "0\n",
      "[0.17717737 0.12372093 0.09578717 0.14912009 0.125472   0.32872245]\n",
      "reward:  8.656558798002292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.17674024 0.12384465 0.09606291 0.1490986  0.12567586 0.3285778 ]\n",
      "reward:  8.331830327328378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.17676656 0.1236398  0.09615459 0.14923699 0.12572323 0.32847884]\n",
      "reward:  8.00201712809137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.17685021 0.12357048 0.09586166 0.14920361 0.12583324 0.32868072]\n",
      "reward:  7.687133319119424\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.17662352 0.12351222 0.09580015 0.14920422 0.12566836 0.3291915 ]\n",
      "reward:  7.387223383781021\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "2\n",
      "[0.17672335 0.12338882 0.0956058  0.14919783 0.12555893 0.32952535]\n",
      "reward:  7.337632014878852\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.17708899 0.12334489 0.09556107 0.14891523 0.12532921 0.3297606 ]\n",
      "reward:  7.0618400137737884\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "2\n",
      "[0.1771114  0.12332947 0.09541868 0.14878185 0.1251928  0.33016577]\n",
      "reward:  7.001404290249461\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.17705171 0.12334998 0.09536603 0.14906138 0.12525627 0.32991472]\n",
      "reward:  6.743899962212677\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.17700599 0.12320011 0.09538908 0.14901073 0.12524918 0.33014488]\n",
      "reward:  6.49501744661379\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "1\n",
      "[0.089381   0.1436246  0.10274754 0.1669146  0.06219304 0.43513918]\n",
      "reward:  12.317231420457501\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "0\n",
      "[0.09175371 0.14557119 0.10508122 0.16838863 0.06456909 0.42463616]\n",
      "reward:  13.162213825926493\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[0.09229543 0.14571734 0.1053959  0.1690935  0.06554397 0.42195386]\n",
      "reward:  13.0664978989294\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.09165023 0.14548567 0.10523955 0.16844267 0.06453755 0.42464426]\n",
      "reward:  12.837150251983273\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[0.09164938 0.14554343 0.10475869 0.1684837  0.06458567 0.42497915]\n",
      "reward:  12.768360469766739\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.09159419 0.14548087 0.10504105 0.16856463 0.06451307 0.4248062 ]\n",
      "reward:  12.461516427919229\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "2\n",
      "[0.09140921 0.14510334 0.10513004 0.16815719 0.06431951 0.42588076]\n",
      "reward:  12.437241368643182\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[0.0919576  0.14555489 0.10471287 0.16832927 0.06451967 0.4249257 ]\n",
      "reward:  12.376940199931393\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "2\n",
      "[0.09159082 0.14550513 0.10482004 0.16820166 0.06436116 0.42552122]\n",
      "reward:  12.35690831258504\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.0918443  0.14534931 0.10477537 0.16834392 0.06442395 0.42526323]\n",
      "reward:  12.298597755813223\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "3\n",
      "[0.0917338  0.14526121 0.10479032 0.16821119 0.06445434 0.42554912]\n",
      "reward:  12.171231185079163\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.09158332 0.14551564 0.10518936 0.16835748 0.06460231 0.42475185]\n",
      "reward:  11.787770745524\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "3\n",
      "[0.0914753  0.14560328 0.10522456 0.16817689 0.0646362  0.42488375]\n",
      "reward:  11.59942893824629\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.09197998 0.14586322 0.10503616 0.16869983 0.06498995 0.4234309 ]\n",
      "reward:  11.372364397689756\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[0.0917685  0.14597161 0.10550538 0.16815697 0.06494816 0.42364934]\n",
      "reward:  10.980951819489272\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[0.09185021 0.14628118 0.10558835 0.1682142  0.06506666 0.4229994 ]\n",
      "reward:  10.566563297156137\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "3\n",
      "[0.09194779 0.14644921 0.10571929 0.16831787 0.06530449 0.42226133]\n",
      "reward:  10.334252235137582\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "2\n",
      "[0.09212349 0.14613359 0.1048443  0.16852435 0.0650901  0.4232842 ]\n",
      "reward:  10.469773826539798\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[0.09198835 0.14624338 0.10582592 0.16833086 0.06533941 0.42227212]\n",
      "reward:  10.061269534411027\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "1\n",
      "[0.09207297 0.14629865 0.10564261 0.1683324  0.06521534 0.422438  ]\n",
      "reward:  9.618720766227963\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.09207343 0.14604987 0.10548199 0.1685177  0.06529046 0.42258662]\n",
      "reward:  9.674890959929213\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[0.09220386 0.14592619 0.10500022 0.16888554 0.06558488 0.42239928]\n",
      "reward:  9.305371723139018\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "1\n",
      "[0.09235659 0.14607915 0.10500207 0.16878316 0.06546511 0.42231393]\n",
      "reward:  8.922804322054859\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "3\n",
      "[0.09230272 0.14587393 0.10513783 0.16861825 0.06537238 0.42269477]\n",
      "reward:  8.79846061551185\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "1\n",
      "[0.09237768 0.14614883 0.10545909 0.1685403  0.06530473 0.42216927]\n",
      "reward:  8.456712698744285\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "3\n",
      "[0.09236502 0.14606063 0.10537536 0.16848338 0.06532269 0.422393  ]\n",
      "reward:  8.327893829143651\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[0.09226435 0.1459735  0.10514081 0.16866031 0.06513837 0.4228227 ]\n",
      "reward:  8.052492089937402\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "3\n",
      "[0.09227926 0.14585449 0.10536575 0.16835529 0.06507075 0.42307442]\n",
      "reward:  7.921630106588299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.09225751 0.14558433 0.10515699 0.16854881 0.06504037 0.42341202]\n",
      "reward:  7.659723433974486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.09202748 0.14546971 0.10521096 0.16823357 0.06487847 0.42417985]\n",
      "reward:  7.399424118040797\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "3\n",
      "[0.09202053 0.14537308 0.10503979 0.16815716 0.06485593 0.42455357]\n",
      "reward:  7.278354032724459\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.09189916 0.14524986 0.10510229 0.16795313 0.06480385 0.42499173]\n",
      "reward:  7.038682074688005\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.09194331 0.14532141 0.10504152 0.16797    0.06468174 0.42504206]\n",
      "reward:  7.088395252058457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.09203722 0.14563996 0.10518324 0.16795729 0.06483221 0.42435014]\n",
      "reward:  6.852851621229215\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[0.09192236 0.14544417 0.10505499 0.16797492 0.06475428 0.42484927]\n",
      "reward:  6.7528102882768835\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.09188068 0.14518468 0.10494147 0.16805688 0.06465978 0.42527652]\n",
      "reward:  6.53057389142336\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "5\n",
      "[0.09170869 0.14528798 0.10496694 0.16801584 0.06462935 0.42539114]\n",
      "reward:  6.3131110239323665\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "2\n",
      "[0.09168399 0.14516157 0.10471998 0.16791806 0.06444579 0.42607057]\n",
      "reward:  6.343382280751109\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "3\n",
      "[0.09170837 0.1452731  0.10477952 0.167955   0.06437083 0.4259132 ]\n",
      "reward:  6.267571347474665\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "1\n",
      "[0.09164616 0.14506796 0.10470695 0.16793536 0.06440289 0.4262407 ]\n",
      "reward:  6.124650621161265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "1\n",
      "[0.091572   0.1451408  0.10465902 0.16782355 0.06431293 0.4264917 ]\n",
      "reward:  5.98075135381121\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.09159636 0.14507687 0.10458977 0.16785096 0.06435521 0.42653078]\n",
      "reward:  5.8005341923255145\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[0.09164295 0.14506081 0.10464174 0.16794975 0.0643862  0.42631856]\n",
      "reward:  5.626037542614363\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "1\n",
      "[0.0915918  0.14504176 0.1045822  0.16793317 0.06441433 0.42643675]\n",
      "reward:  5.505970617042481\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "1\n",
      "[0.09159268 0.14506185 0.10464416 0.16794297 0.06443104 0.42632726]\n",
      "reward:  5.387797585858694\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "3\n",
      "[0.09167733 0.14507501 0.10469276 0.16801663 0.06445876 0.42607945]\n",
      "reward:  5.3302764693249705\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "1\n",
      "[0.09170146 0.14509727 0.10469787 0.16799411 0.06446588 0.42604345]\n",
      "reward:  5.219283279124449\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "1\n",
      "[0.0916747  0.14512406 0.10468589 0.16800703 0.06448393 0.4260244 ]\n",
      "reward:  5.108432211298732\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "4\n",
      "[0.09173112 0.1451123  0.10471345 0.16801651 0.06454235 0.42588425]\n",
      "reward:  5.207303465311497\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "5\n",
      "[0.09167293 0.14509307 0.10469215 0.16799594 0.06448381 0.42606214]\n",
      "reward:  5.082202084094764\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "1\n",
      "[0.01111205 0.18382394 0.1324617  0.20089875 0.0044876  0.46721599]\n",
      "reward:  12.291007910104002\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "2\n",
      "[0.01244873 0.1868495  0.13564028 0.2031463  0.00521077 0.45670447]\n",
      "reward:  12.212760018017468\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.01258306 0.18680207 0.13578385 0.2034131  0.0053252  0.4560927 ]\n",
      "reward:  12.106687131729963\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[0.01236588 0.18638836 0.13488202 0.2032225  0.00518875 0.4579525 ]\n",
      "reward:  11.918780911423552\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.01232272 0.18621638 0.13484144 0.20300795 0.00514794 0.45846352]\n",
      "reward:  11.662817001039114\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.01241155 0.1864351  0.13526577 0.20281874 0.00517781 0.45789105]\n",
      "reward:  11.351739962908285\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[0.01242621 0.18633014 0.13492297 0.2030022  0.00517765 0.45814082]\n",
      "reward:  10.99963258686794\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.01244511 0.18675144 0.13508752 0.20295192 0.00521481 0.45754924]\n",
      "reward:  10.620067652710876\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "1\n",
      "[0.01249133 0.18733172 0.13540171 0.20305116 0.00525609 0.45646805]\n",
      "reward:  10.067644722525\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "1\n",
      "[0.01252109 0.18715909 0.1353612  0.2032139  0.00526199 0.4564828 ]\n",
      "reward:  9.556447058663256\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.01250002 0.18716459 0.13544083 0.20323691 0.00525202 0.4564056 ]\n",
      "reward:  9.255624393934468\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "3\n",
      "[0.01249566 0.18721774 0.1354079  0.20320736 0.00523557 0.4564358 ]\n",
      "reward:  9.226681772576363\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "2\n",
      "[0.01250924 0.18721907 0.13571452 0.20322402 0.00524991 0.4560832 ]\n",
      "reward:  9.177705733267612\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[0.01254081 0.18704313 0.13540278 0.20327467 0.0052542  0.45648432]\n",
      "reward:  9.154902434113692\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "1\n",
      "[0.01253204 0.18729198 0.13572139 0.2033052  0.00526384 0.45588556]\n",
      "reward:  8.747840019508235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.01249487 0.18731174 0.13574953 0.20307042 0.00524584 0.45612758]\n",
      "reward:  8.47454341276853\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[0.01248438 0.18716323 0.13561076 0.20306624 0.0052368  0.45643854]\n",
      "reward:  8.202361486208337\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.01247833 0.18711834 0.1353787  0.20307414 0.00522779 0.45672262]\n",
      "reward:  8.152705834383402\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[0.0125101  0.18715905 0.13588259 0.20301479 0.00524449 0.45618898]\n",
      "reward:  7.888608716156406\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "1\n",
      "[0.01247516 0.1866628  0.1355949  0.20291805 0.0052131  0.45713598]\n",
      "reward:  7.597050277037198\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "1\n",
      "[0.01242702 0.18657844 0.13530706 0.20282255 0.00518609 0.4576788 ]\n",
      "reward:  7.319793122314924\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.01239046 0.18639855 0.13528915 0.20278996 0.0051665  0.45796528]\n",
      "reward:  7.101177882880892\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.0123538  0.18629292 0.13506465 0.20279142 0.00515159 0.4583456 ]\n",
      "reward:  6.886609368933845\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.01228082 0.18603718 0.13477106 0.2026598  0.00510922 0.45914197]\n",
      "reward:  6.675757458017445\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "3\n",
      "[0.01225823 0.18589903 0.13479267 0.20265578 0.00509218 0.45930204]\n",
      "reward:  6.629442401563135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.01230613 0.18627498 0.13499005 0.20265137 0.00511105 0.45866647]\n",
      "reward:  6.430326735391903\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.01228637 0.18597184 0.13478516 0.20273216 0.00509497 0.45912948]\n",
      "reward:  6.376828319291394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.01225838 0.18623778 0.13514537 0.20269093 0.00510066 0.45856693]\n",
      "reward:  6.186231127313744\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.01219351 0.18570606 0.13476637 0.20268321 0.00506148 0.45958942]\n",
      "reward:  6.1279980255375905\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "3\n",
      "[0.01220997 0.18590388 0.13495457 0.20264186 0.0050559  0.45923385]\n",
      "reward:  6.066190437315399\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "3\n",
      "[0.01219885 0.18604137 0.13490984 0.20283298 0.00506598 0.45895106]\n",
      "reward:  5.997305873926556\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.01217643 0.18614909 0.13494092 0.2026367  0.00505171 0.4590451 ]\n",
      "reward:  5.824685633124462\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "2\n",
      "[0.01217446 0.18593292 0.13494396 0.20268723 0.00505169 0.4592097 ]\n",
      "reward:  5.844843828610337\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[0.0121931  0.18595566 0.13483773 0.2027503  0.00505536 0.45920792]\n",
      "reward:  5.677028072260729\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.01216233 0.18579844 0.13475461 0.20262438 0.00503974 0.45962048]\n",
      "reward:  5.510246783023262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "2\n",
      "[0.01216281 0.18574429 0.1348612  0.20258442 0.00504124 0.45960608]\n",
      "reward:  5.518749999225514\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "2\n",
      "[0.0121722  0.18570767 0.13485895 0.20260929 0.00504445 0.4596074 ]\n",
      "reward:  5.52500830573803\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.01218153 0.18570161 0.13489838 0.20261796 0.00504821 0.45955232]\n",
      "reward:  5.3656798794303455\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "1\n",
      "[0.01220762 0.18562002 0.13490471 0.20267113 0.0050612  0.45953533]\n",
      "reward:  5.252833261276421\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.01220385 0.18563278 0.13491613 0.20266372 0.0050602  0.45952335]\n",
      "reward:  5.105710462289505\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "1\n",
      "[0.01221597 0.18571322 0.13494124 0.20269938 0.00506911 0.45936105]\n",
      "reward:  5.00304916310004\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.01225693 0.18575774 0.13505052 0.20276588 0.00509158 0.45907733]\n",
      "reward:  4.8693799905823445\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "3\n",
      "[0.01225427 0.18577063 0.13507813 0.20275316 0.00509054 0.45905322]\n",
      "reward:  4.835954503444073\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[0.0122638  0.18577194 0.13506639 0.20275526 0.0050943  0.4590483 ]\n",
      "reward:  4.711026109327023\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.01227621 0.1857887  0.13511346 0.20276695 0.00510126 0.45895335]\n",
      "reward:  4.588250735824756\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[0.01231616 0.18584552 0.13517521 0.20281027 0.0051215  0.45873132]\n",
      "reward:  4.4702639851454515\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.01231092 0.18581633 0.13517392 0.20280956 0.00511841 0.4587708 ]\n",
      "reward:  4.356834743764245\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.01234217 0.18587518 0.13522631 0.20284125 0.00513622 0.45857882]\n",
      "reward:  4.247822833096039\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "3\n",
      "[0.01234922 0.18590662 0.13526177 0.20284285 0.00514023 0.45849937]\n",
      "reward:  4.220202095787743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.01235735 0.18591633 0.13528043 0.20285316 0.00514496 0.45844778]\n",
      "reward:  4.118946813589369\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "3\n",
      "[0.00300399 0.18507075 0.05337377 0.20751862 0.00085902 0.5501739 ]\n",
      "reward:  13.151700561612998\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.00367612 0.18965779 0.05802518 0.21244338 0.00112487 0.5350727 ]\n",
      "reward:  13.026867836287325\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "2\n",
      "[0.0035714  0.188576   0.05726143 0.21143387 0.0010766  0.53808063]\n",
      "reward:  13.038944488580277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "1\n",
      "[0.00362418 0.1885171  0.05747676 0.21158919 0.00109541 0.5376974 ]\n",
      "reward:  12.157387291607163\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "1\n",
      "[0.00352991 0.18825428 0.05671107 0.21090017 0.00105744 0.53954715]\n",
      "reward:  11.361694416884307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "3\n",
      "[0.00352402 0.18831007 0.05661482 0.21083641 0.0010515  0.5396632 ]\n",
      "reward:  11.285748537348223\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00352753 0.18847671 0.05678744 0.21100733 0.00105766 0.5391433 ]\n",
      "reward:  11.122392364698308\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "0\n",
      "[0.0035311  0.18869177 0.05690632 0.21111734 0.00106046 0.5386931 ]\n",
      "reward:  11.771980099114007\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00353572 0.18839185 0.05699823 0.21136776 0.00106481 0.53864163]\n",
      "reward:  11.525022484863893\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "1\n",
      "[0.00353431 0.18888842 0.05700993 0.21115217 0.00106211 0.5383531 ]\n",
      "reward:  10.840645048724404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.00353774 0.18915015 0.05700271 0.21113572 0.00106391 0.5381098 ]\n",
      "reward:  10.708424826730056\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00353993 0.18935448 0.05703924 0.21079676 0.0010622  0.5382074 ]\n",
      "reward:  10.44059841084692\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "1\n",
      "[0.00355661 0.18954499 0.05705005 0.21118064 0.00107321 0.5375945 ]\n",
      "reward:  9.884955308088681\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "1\n",
      "[0.00357233 0.1898316  0.05717766 0.2113068  0.00107621 0.53703547]\n",
      "reward:  9.372909735196538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[0.00357733 0.18981926 0.05724074 0.21133378 0.0010771  0.5369518 ]\n",
      "reward:  9.139788137837824\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.00356205 0.18971851 0.05715379 0.21112396 0.00107444 0.5373672 ]\n",
      "reward:  8.895477120527023\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.00356557 0.18994    0.05719148 0.21121515 0.00107525 0.5370125 ]\n",
      "reward:  8.638045598549487\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "3\n",
      "[0.00358292 0.19003688 0.05724209 0.21120045 0.00107823 0.5368594 ]\n",
      "reward:  8.5327297186529\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.00359187 0.1906677  0.05712363 0.2114049  0.00108557 0.53612626]\n",
      "reward:  8.411574214920199\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.00358183 0.19083512 0.05735653 0.21144447 0.00108314 0.53569895]\n",
      "reward:  8.163537736323478\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00358466 0.19063799 0.0572632  0.21142986 0.00108042 0.5360039 ]\n",
      "reward:  7.908706810726247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "1\n",
      "[0.00358875 0.19037764 0.0573322  0.2114834  0.00108192 0.53613615]\n",
      "reward:  7.615895552852699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.00357678 0.19009784 0.05714957 0.21137093 0.00107582 0.53672904]\n",
      "reward:  7.3848664776088135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.00355138 0.1897375  0.05696119 0.21118909 0.00106387 0.5374969 ]\n",
      "reward:  7.157428543833162\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "3\n",
      "[0.00353874 0.18928126 0.0567915  0.21108791 0.00105662 0.53824395]\n",
      "reward:  7.0580526288340835\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "3\n",
      "[0.00354443 0.18950225 0.05679743 0.21136557 0.00106338 0.53772694]\n",
      "reward:  6.9523760882720165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "3\n",
      "[0.00353091 0.18903038 0.05683616 0.2109816  0.00105859 0.5385623 ]\n",
      "reward:  6.837860474369493\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.00352874 0.18943624 0.05691936 0.2108165  0.00106021 0.53823894]\n",
      "reward:  6.63876920711613\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "3\n",
      "[0.00350393 0.18910721 0.05665649 0.21074238 0.00104807 0.538942  ]\n",
      "reward:  6.523085879467385\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[0.00352306 0.1889904  0.05675944 0.21071076 0.00104903 0.5389674 ]\n",
      "reward:  6.3356057985761405\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "3\n",
      "[0.00349185 0.18891105 0.05654407 0.2105566  0.00103942 0.5394571 ]\n",
      "reward:  6.223283121362873\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.00349349 0.18852034 0.05662702 0.21057172 0.00104088 0.53974664]\n",
      "reward:  6.0476148230925775\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "1\n",
      "[0.00348189 0.18853897 0.05646516 0.21052293 0.00103594 0.539955  ]\n",
      "reward:  5.898653793943411\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[0.00346571 0.18837522 0.05639638 0.21032694 0.00102963 0.5404061 ]\n",
      "reward:  5.798018574983714\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.00346204 0.18821436 0.05637768 0.21054144 0.00103152 0.54037297]\n",
      "reward:  5.644557336020073\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "3\n",
      "[0.0034439  0.18809922 0.0562395  0.21043965 0.00102395 0.5407537 ]\n",
      "reward:  5.547865735242721\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "5\n",
      "[0.00344147 0.1880344  0.05609561 0.21040753 0.0010234  0.54099756]\n",
      "reward:  5.404199812890657\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.00344257 0.18794186 0.05612533 0.21033101 0.00102269 0.5411365 ]\n",
      "reward:  5.262622507386099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "3\n",
      "[0.00343773 0.1879303  0.05613533 0.21026349 0.00102058 0.5412126 ]\n",
      "reward:  5.176334173884262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00343984 0.18797235 0.05609619 0.21034065 0.00102177 0.54112923]\n",
      "reward:  5.0456369766867\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "1\n",
      "[0.00345203 0.18792617 0.05619285 0.21031529 0.0010252  0.5410885 ]\n",
      "reward:  4.9500081497266315\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "3\n",
      "[0.00345023 0.1878881  0.05618915 0.21031357 0.00102475 0.54113424]\n",
      "reward:  4.872579880448985\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.00345224 0.18788053 0.05620394 0.21035868 0.00102527 0.54107934]\n",
      "reward:  4.7566715054145146\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "3\n",
      "[0.00345255 0.18786857 0.05620291 0.21037006 0.00102584 0.5410802 ]\n",
      "reward:  4.682402461221433\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00345297 0.18791914 0.05620459 0.21038657 0.00102607 0.5410107 ]\n",
      "reward:  4.573909595673813\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[0.00347017 0.18799055 0.0562933  0.21046758 0.00103278 0.5407457 ]\n",
      "reward:  4.467257905259641\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.00347278 0.18801326 0.05631186 0.21047568 0.00103403 0.5406924 ]\n",
      "reward:  4.363644065018618\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.00347947 0.18802127 0.05634968 0.21049331 0.00103632 0.5406199 ]\n",
      "reward:  4.263067207541908\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[0.00349453 0.18810119 0.05642796 0.21057284 0.00104175 0.54036176]\n",
      "reward:  4.1655408382455885\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "5\n",
      "[0.00349303 0.18806402 0.05641551 0.21055949 0.00104096 0.5404269 ]\n",
      "reward:  4.071053638443871\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "1\n",
      "[4.5748614e-03 1.2948643e-01 7.0696324e-03 2.2009173e-01 4.1019628e-04\n",
      " 6.3836724e-01]\n",
      "reward:  12.367524423149478\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[5.4685054e-03 1.3596672e-01 8.2858037e-03 2.2580206e-01 5.4148998e-04\n",
      " 6.2393546e-01]\n",
      "reward:  12.268437297271275\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[5.3832470e-03 1.3574095e-01 8.1754867e-03 2.2520824e-01 5.2903139e-04\n",
      " 6.2496305e-01]\n",
      "reward:  12.108194258011382\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "1\n",
      "[5.3559849e-03 1.3507818e-01 8.1433859e-03 2.2482426e-01 5.2396598e-04\n",
      " 6.2607419e-01]\n",
      "reward:  11.359177562599111\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "1\n",
      "[5.3338772e-03 1.3502561e-01 8.1141070e-03 2.2469664e-01 5.2018207e-04\n",
      " 6.2630951e-01]\n",
      "reward:  10.680097885646347\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "1\n",
      "[5.3346665e-03 1.3483946e-01 8.1156362e-03 2.2469690e-01 5.1975122e-04\n",
      " 6.2649357e-01]\n",
      "reward:  10.074905091146949\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[5.3667440e-03 1.3533372e-01 8.1494134e-03 2.2485080e-01 5.2522344e-04\n",
      " 6.2577415e-01]\n",
      "reward:  9.92166578736896\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[5.3747501e-03 1.3536243e-01 8.1734648e-03 2.2495395e-01 5.2676420e-04\n",
      " 6.2560862e-01]\n",
      "reward:  9.741421579063159\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "5\n",
      "[5.3766561e-03 1.3537197e-01 8.1870109e-03 2.2499746e-01 5.2762736e-04\n",
      " 6.2553930e-01]\n",
      "reward:  9.528270871103727\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[5.3974013e-03 1.3536772e-01 8.2026059e-03 2.2491246e-01 5.3018570e-04\n",
      " 6.2558973e-01]\n",
      "reward:  9.289536672806541\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "1\n",
      "[5.3835227e-03 1.3540094e-01 8.1899213e-03 2.2475226e-01 5.2950322e-04\n",
      " 6.2574387e-01]\n",
      "reward:  8.858369493586155\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[5.3881342e-03 1.3558005e-01 8.2109803e-03 2.2484189e-01 5.3062278e-04\n",
      " 6.2544835e-01]\n",
      "reward:  8.627424847499837\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "5\n",
      "[5.4026586e-03 1.3568664e-01 8.2177231e-03 2.2497886e-01 5.3332013e-04\n",
      " 6.2518078e-01]\n",
      "reward:  8.389981838433206\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[5.4009971e-03 1.3550754e-01 8.2031786e-03 2.2499029e-01 5.3416402e-04\n",
      " 6.2536383e-01]\n",
      "reward:  8.144537413491292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[5.3849751e-03 1.3520010e-01 8.1715193e-03 2.2485860e-01 5.3127727e-04\n",
      " 6.2585354e-01]\n",
      "reward:  7.8953453152152235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[5.3807180e-03 1.3515395e-01 8.1636226e-03 2.2487949e-01 5.3065026e-04\n",
      " 6.2589157e-01]\n",
      "reward:  7.6457871526045444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "3\n",
      "[5.3519984e-03 1.3473369e-01 8.1010303e-03 2.2491430e-01 5.2600587e-04\n",
      " 6.2637299e-01]\n",
      "reward:  7.603908791551997\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "3\n",
      "[5.3456668e-03 1.3484453e-01 8.1278151e-03 2.2454111e-01 5.2506803e-04\n",
      " 6.2661576e-01]\n",
      "reward:  7.551108080391892\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[5.3654495e-03 1.3535319e-01 8.1407102e-03 2.2498125e-01 5.2627415e-04\n",
      " 6.2563306e-01]\n",
      "reward:  7.313119455236331\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[5.3377682e-03 1.3522489e-01 8.0898292e-03 2.2497033e-01 5.2141730e-04\n",
      " 6.2585574e-01]\n",
      "reward:  7.074567003056057\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "3\n",
      "[5.3315111e-03 1.3501151e-01 8.0800802e-03 2.2496670e-01 5.1980896e-04\n",
      " 6.2609041e-01]\n",
      "reward:  7.008912374829973\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "1\n",
      "[5.3277225e-03 1.3488418e-01 8.0852732e-03 2.2494602e-01 5.1846879e-04\n",
      " 6.2623829e-01]\n",
      "reward:  6.802138576647925\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "3\n",
      "[5.2856724e-03 1.3467750e-01 8.0193430e-03 2.2462851e-01 5.1249447e-04\n",
      " 6.2687647e-01]\n",
      "reward:  6.730475750820063\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[5.3121354e-03 1.3487212e-01 8.0495412e-03 2.2459978e-01 5.1555224e-04\n",
      " 6.2665093e-01]\n",
      "reward:  6.529352577598885\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "3\n",
      "[5.2715731e-03 1.3443008e-01 7.9970183e-03 2.2444062e-01 5.1008270e-04\n",
      " 6.2735063e-01]\n",
      "reward:  6.453758273893769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[5.2646012e-03 1.3455324e-01 8.0312332e-03 2.2445269e-01 5.1136746e-04\n",
      " 6.2718683e-01]\n",
      "reward:  6.262372453036239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[5.2364534e-03 1.3426180e-01 7.9838764e-03 2.2432047e-01 5.0691119e-04\n",
      " 6.2769049e-01]\n",
      "reward:  6.073051434440252\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[5.2050641e-03 1.3412173e-01 7.9442300e-03 2.2410029e-01 5.0253555e-04\n",
      " 6.2812614e-01]\n",
      "reward:  5.889954095117566\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "5\n",
      "[5.1996484e-03 1.3396341e-01 7.9279039e-03 2.2392243e-01 5.0065701e-04\n",
      " 6.2848592e-01]\n",
      "reward:  5.713261194720205\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[5.1964340e-03 1.3382278e-01 7.9237437e-03 2.2394362e-01 5.0102553e-04\n",
      " 6.2861240e-01]\n",
      "reward:  5.543150754594251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "1\n",
      "[5.2029770e-03 1.3387276e-01 7.9332711e-03 2.2394145e-01 5.0216669e-04\n",
      " 6.2854737e-01]\n",
      "reward:  5.426672995171818\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[5.2099745e-03 1.3386883e-01 7.9472363e-03 2.2393137e-01 5.0313369e-04\n",
      " 6.2853944e-01]\n",
      "reward:  5.273282529817294\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "3\n",
      "[5.2157654e-03 1.3387837e-01 7.9510594e-03 2.2398329e-01 5.0374406e-04\n",
      " 6.2846774e-01]\n",
      "reward:  5.221649581081198\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[5.2157138e-03 1.3390569e-01 7.9542613e-03 2.2402576e-01 5.0369819e-04\n",
      " 6.2839496e-01]\n",
      "reward:  5.079104578363567\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "3\n",
      "[5.2166390e-03 1.3393353e-01 7.9552019e-03 2.2404076e-01 5.0419057e-04\n",
      " 6.2834972e-01]\n",
      "reward:  5.026882923633544\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[5.2206493e-03 1.3396521e-01 7.9583414e-03 2.2405504e-01 5.0458161e-04\n",
      " 6.2829626e-01]\n",
      "reward:  4.892869753126415\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "5\n",
      "[5.2397246e-03 1.3403678e-01 7.9812109e-03 2.2412013e-01 5.0717220e-04\n",
      " 6.2811494e-01]\n",
      "reward:  4.76161640799696\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[5.2549629e-03 1.3408285e-01 8.0048488e-03 2.2417957e-01 5.0956482e-04\n",
      " 6.2796825e-01]\n",
      "reward:  4.635522378413168\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[5.2706236e-03 1.3415125e-01 8.0247745e-03 2.2423854e-01 5.1171059e-04\n",
      " 6.2780315e-01]\n",
      "reward:  4.514367159516128\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[5.2779671e-03 1.3418406e-01 8.0343504e-03 2.2427411e-01 5.1276793e-04\n",
      " 6.2771666e-01]\n",
      "reward:  4.398006302425456\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[5.2787047e-03 1.3419430e-01 8.0348635e-03 2.2430059e-01 5.1292905e-04\n",
      " 6.2767857e-01]\n",
      "reward:  4.286280958070914\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "3\n",
      "[5.2975076e-03 1.3425225e-01 8.0599198e-03 2.2441171e-01 5.1574776e-04\n",
      " 6.2746280e-01]\n",
      "reward:  4.24859526375234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[5.2997344e-03 1.3426414e-01 8.0625853e-03 2.2441645e-01 5.1605364e-04\n",
      " 6.2744105e-01]\n",
      "reward:  4.145343947636215\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[5.3122421e-03 1.3434660e-01 8.0813095e-03 2.2446479e-01 5.1804964e-04\n",
      " 6.2727702e-01]\n",
      "reward:  4.04434585587188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "5\n",
      "[5.3276466e-03 1.3445571e-01 8.1058061e-03 2.2453393e-01 5.2039337e-04\n",
      " 6.2705648e-01]\n",
      "reward:  3.947381217767417\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "1\n",
      "[5.3311731e-03 1.3443369e-01 8.1108166e-03 2.2454859e-01 5.2089064e-04\n",
      " 6.2705493e-01]\n",
      "reward:  3.8986166987774316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "5\n",
      "[5.3294208e-03 1.3442573e-01 8.1078336e-03 2.2455181e-01 5.2057410e-04\n",
      " 6.2706465e-01]\n",
      "reward:  3.8095167841905826\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "1\n",
      "[5.3419624e-03 1.3449852e-01 8.1250379e-03 2.2459708e-01 5.2247889e-04\n",
      " 6.2691498e-01]\n",
      "reward:  3.763397729752733\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[5.3512575e-03 1.3455436e-01 8.1379339e-03 2.2464517e-01 5.2376924e-04\n",
      " 6.2678760e-01]\n",
      "reward:  3.68043769043707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[5.3634709e-03 1.3462782e-01 8.1541277e-03 2.2470571e-01 5.2554504e-04\n",
      " 6.2662327e-01]\n",
      "reward:  3.5995810820785334\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "1\n",
      "[0.00774865 0.15329927 0.008021   0.23983145 0.00070279 0.59039694]\n",
      "reward:  12.32122811802138\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "3\n",
      "[0.00905855 0.15761887 0.00934801 0.2439724  0.00090278 0.5790993 ]\n",
      "reward:  12.246442072768149\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.00913337 0.15824325 0.00945102 0.2445049  0.00092101 0.57774645]\n",
      "reward:  12.149840153114912\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00890757 0.15738417 0.00919938 0.24374579 0.00088552 0.5798776 ]\n",
      "reward:  11.969955781972935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.00884621 0.15736312 0.00913525 0.24344197 0.00087482 0.58033866]\n",
      "reward:  11.7205358286789\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[0.00885342 0.15753008 0.00916084 0.24354053 0.00087558 0.58003956]\n",
      "reward:  11.414299192167102\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "1\n",
      "[0.00881749 0.15739018 0.00915349 0.24337877 0.00087023 0.58038986]\n",
      "reward:  10.756019640104533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.00887901 0.15775584 0.00919569 0.24350874 0.00087983 0.5797809 ]\n",
      "reward:  10.450489412739058\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[0.00889645 0.15796176 0.00922361 0.24354209 0.00088406 0.579492  ]\n",
      "reward:  10.128168262852142\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "5\n",
      "[0.00889186 0.15806347 0.00923191 0.24343006 0.00088356 0.5794992 ]\n",
      "reward:  9.78810924742116\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "5\n",
      "[0.00887155 0.15804735 0.00921879 0.24317564 0.00088178 0.57980496]\n",
      "reward:  9.439234787713168\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "5\n",
      "[0.00892855 0.15826216 0.00926081 0.24328321 0.00088869 0.5793766 ]\n",
      "reward:  9.088599971782351\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[0.00892816 0.15828395 0.0092282  0.2431637  0.00088708 0.57950884]\n",
      "reward:  8.741731447275647\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "3\n",
      "[0.00894991 0.15832339 0.00924012 0.24310042 0.00088859 0.5794976 ]\n",
      "reward:  8.65579729787665\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "3\n",
      "[0.00898848 0.158754   0.00927965 0.24374977 0.00089371 0.57833445]\n",
      "reward:  8.554155685263533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[0.00896516 0.15855154 0.00925662 0.24365191 0.00089248 0.5786823 ]\n",
      "reward:  8.235232323729502\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[0.00894707 0.15811516 0.00921198 0.2435835  0.00088751 0.5792548 ]\n",
      "reward:  7.9201966416608025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "1\n",
      "[0.00892637 0.1580498  0.0091792  0.24342796 0.00088261 0.57953405]\n",
      "reward:  7.6589632194736845\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "3\n",
      "[0.00890361 0.1578694  0.00915764 0.24334055 0.00087787 0.579851  ]\n",
      "reward:  7.559248566440436\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "1\n",
      "[0.00891835 0.15817185 0.00919069 0.24359503 0.00087945 0.5792446 ]\n",
      "reward:  7.321028210523865\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "1\n",
      "[0.0088866  0.15797243 0.00915083 0.24341151 0.00087313 0.5797055 ]\n",
      "reward:  7.086666032511409\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00886542 0.1579059  0.00912475 0.2433019  0.00086989 0.57993215]\n",
      "reward:  6.861967940914503\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[0.0088329  0.15776016 0.00909253 0.24314347 0.00086491 0.58030605]\n",
      "reward:  6.643512480853203\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[0.00879789 0.15740637 0.00905321 0.24313496 0.00085961 0.580748  ]\n",
      "reward:  6.431599087643537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.00877508 0.15735207 0.00902342 0.24293615 0.00085552 0.5810577 ]\n",
      "reward:  6.22688213078623\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "1\n",
      "[0.00872454 0.15699673 0.00897008 0.24283276 0.00084805 0.58162796]\n",
      "reward:  6.0688156651909475\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[0.00871685 0.15697958 0.0089572  0.2427958  0.00084721 0.58170336]\n",
      "reward:  5.885574469313568\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[0.00869584 0.15688376 0.00895574 0.24272555 0.0008464  0.58189267]\n",
      "reward:  5.708506713143441\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "3\n",
      "[0.00867683 0.15679441 0.0089443  0.24273069 0.00084401 0.5820098 ]\n",
      "reward:  5.6516718058940505\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[0.00868064 0.15675019 0.00894607 0.24274975 0.00084415 0.58202916]\n",
      "reward:  5.488068347540707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "3\n",
      "[0.00867232 0.15672144 0.00894198 0.24268728 0.00084323 0.58213377]\n",
      "reward:  5.429851041027692\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "3\n",
      "[0.00868903 0.15679811 0.00896432 0.24273829 0.00084656 0.58196366]\n",
      "reward:  5.369935081558547\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[0.00869194 0.15685439 0.00896963 0.2427538  0.00084693 0.5818833 ]\n",
      "reward:  5.22019812199311\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "5\n",
      "[0.00869063 0.15682358 0.00896711 0.24275444 0.00084697 0.5819173 ]\n",
      "reward:  5.073779587343721\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.0086902  0.15687597 0.00896737 0.2427539  0.00084731 0.58186525]\n",
      "reward:  4.933180142534916\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "1\n",
      "[0.00873191 0.15701939 0.00900823 0.24285987 0.00085336 0.58152723]\n",
      "reward:  4.845051658990243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[0.00873986 0.1570613  0.00901855 0.2429013  0.00085479 0.5814243 ]\n",
      "reward:  4.717435036999836\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[0.00874717 0.15708211 0.00902742 0.24292815 0.00085599 0.5813592 ]\n",
      "reward:  4.593826452977022\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "3\n",
      "[0.00877628 0.15719023 0.00905711 0.2429938  0.00086043 0.5811221 ]\n",
      "reward:  4.548550765988386\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[0.00877628 0.15719023 0.00905711 0.2429938  0.00086043 0.5811221 ]\n",
      "reward:  4.434424174536851\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[0.00877376 0.15717594 0.00905294 0.24298061 0.00085998 0.5811568 ]\n",
      "reward:  4.322764086652301\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "1\n",
      "[0.00878144 0.1572023  0.0090604  0.24299458 0.00086127 0.5811    ]\n",
      "reward:  4.258301875304675\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "3\n",
      "[0.00881171 0.15729071 0.00909014 0.24307418 0.00086617 0.5808671 ]\n",
      "reward:  4.218756924177775\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[0.00881171 0.15729071 0.00909014 0.24307418 0.00086617 0.5808671 ]\n",
      "reward:  4.119687861138133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "1\n",
      "[0.00882637 0.15734994 0.00910623 0.24311417 0.00086865 0.58073467]\n",
      "reward:  4.060752381385883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "5\n",
      "[0.00882637 0.15734994 0.00910623 0.24311417 0.00086865 0.58073467]\n",
      "reward:  3.968288196189804\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[0.00885311 0.15747294 0.00913563 0.24319233 0.00087334 0.58047265]\n",
      "reward:  3.878254628990251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[0.0088586  0.15747744 0.00914111 0.24321204 0.00087426 0.5804366 ]\n",
      "reward:  3.7914084425996504\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "1\n",
      "[0.00885741 0.15747623 0.00914044 0.24319315 0.00087413 0.5804587 ]\n",
      "reward:  3.742445009060302\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "1\n",
      "[0.00888713 0.1575693  0.00917312 0.24328144 0.00087916 0.5802098 ]\n",
      "reward:  3.694172186038084\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "1\n",
      "[3.7777843e-04 1.7916852e-01 3.2347822e-04 9.7569384e-02 1.4598331e-05\n",
      " 7.2254616e-01]\n",
      "reward:  12.36460768679905\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[5.1425974e-04 1.8516752e-01 4.4072937e-04 1.0466582e-01 2.2750717e-05\n",
      " 7.0918888e-01]\n",
      "reward:  12.263938125796548\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[5.0115562e-04 1.8519114e-01 4.3001433e-04 1.0406228e-01 2.1975167e-05\n",
      " 7.0979345e-01]\n",
      "reward:  12.102063231221152\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[4.9557816e-04 1.8523404e-01 4.2548135e-04 1.0377231e-01 2.1611302e-05\n",
      " 7.1005094e-01]\n",
      "reward:  11.864703655900582\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "1\n",
      "[4.91745130e-04 1.85288057e-01 4.23403748e-04 1.03484794e-01\n",
      " 2.14099728e-05 7.10290551e-01]\n",
      "reward:  11.152378292290795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[4.9043697e-04 1.8526785e-01 4.2264725e-04 1.0342460e-01 2.1300015e-05\n",
      " 7.1037310e-01]\n",
      "reward:  10.888611711978495\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[4.93868021e-04 1.85607538e-01 4.25415899e-04 1.03524774e-01\n",
      " 2.15234068e-05 7.09926844e-01]\n",
      "reward:  10.596193142498032\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[4.9496174e-04 1.8564612e-01 4.2655127e-04 1.0358461e-01 2.1638605e-05\n",
      " 7.0982611e-01]\n",
      "reward:  10.272794102809199\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[4.95333283e-04 1.85870498e-01 4.27129795e-04 1.03550345e-01\n",
      " 2.16851622e-05 7.09635079e-01]\n",
      "reward:  9.929387845411053\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[4.9587246e-04 1.8610983e-01 4.2728824e-04 1.0351636e-01 2.1768861e-05\n",
      " 7.0942891e-01]\n",
      "reward:  9.575220083655486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[4.9853360e-04 1.8614851e-01 4.2926412e-04 1.0357513e-01 2.1946307e-05\n",
      " 7.0932657e-01]\n",
      "reward:  9.217960497828878\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[4.9964886e-04 1.8591079e-01 4.3033049e-04 1.0366173e-01 2.2008095e-05\n",
      " 7.0947546e-01]\n",
      "reward:  8.863658870823043\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[4.97674453e-04 1.85883582e-01 4.28637984e-04 1.03615426e-01\n",
      " 2.18645491e-05 7.09552765e-01]\n",
      "reward:  8.516860517705464\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[4.97334229e-04 1.85537457e-01 4.26910439e-04 1.03586525e-01\n",
      " 2.17785055e-05 7.09930062e-01]\n",
      "reward:  8.180802158893593\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[4.9359415e-04 1.8515962e-01 4.2343096e-04 1.0342512e-01 2.1524002e-05\n",
      " 7.1047676e-01]\n",
      "reward:  7.857646797464726\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "5\n",
      "[4.88909660e-04 1.84869453e-01 4.19277989e-04 1.03282034e-01\n",
      " 2.12589111e-05 7.10919023e-01]\n",
      "reward:  7.548700228646127\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[4.8674224e-04 1.8467854e-01 4.1649825e-04 1.0320075e-01 2.1074995e-05\n",
      " 7.1119636e-01]\n",
      "reward:  7.254623152306621\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[4.8117276e-04 1.8445845e-01 4.1178986e-04 1.0294453e-01 2.0725231e-05\n",
      " 7.1168327e-01]\n",
      "reward:  6.975594279349439\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[4.78859438e-04 1.84378892e-01 4.09708708e-04 1.02824666e-01\n",
      " 2.05690176e-05 7.11887360e-01]\n",
      "reward:  6.711446923899732\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "1\n",
      "[4.73832566e-04 1.84273437e-01 4.05296771e-04 1.02538675e-01\n",
      " 2.02519332e-05 7.12288499e-01]\n",
      "reward:  6.5491889185080705\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[4.70994477e-04 1.84144363e-01 4.03311045e-04 1.02433555e-01\n",
      " 2.01015573e-05 7.12527633e-01]\n",
      "reward:  6.317119774095172\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[4.6946784e-04 1.8405865e-01 4.0191144e-04 1.0234367e-01 2.0001484e-05\n",
      " 7.1270621e-01]\n",
      "reward:  6.095618219524245\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[4.6820426e-04 1.8385141e-01 4.0057822e-04 1.0224106e-01 1.9911577e-05\n",
      " 7.1301889e-01]\n",
      "reward:  5.885777485066339\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "3\n",
      "[4.6792475e-04 1.8385811e-01 4.0057511e-04 1.0225241e-01 1.9910622e-05\n",
      " 7.1300107e-01]\n",
      "reward:  5.854381510421066\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[4.6857045e-04 1.8394756e-01 4.0134121e-04 1.0229568e-01 1.9960538e-05\n",
      " 7.1286678e-01]\n",
      "reward:  5.661725906126459\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[4.6852740e-04 1.8383823e-01 4.0158743e-04 1.0228309e-01 1.9951634e-05\n",
      " 7.1298856e-01]\n",
      "reward:  5.474921740859669\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "3\n",
      "[4.6845991e-04 1.8374318e-01 4.0162192e-04 1.0228711e-01 1.9954412e-05\n",
      " 7.1307969e-01]\n",
      "reward:  5.4409897722950715\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "1\n",
      "[4.6873509e-04 1.8385524e-01 4.0177730e-04 1.0230030e-01 1.9975685e-05\n",
      " 7.1295404e-01]\n",
      "reward:  5.347474743885784\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[4.67819977e-04 1.83685362e-01 4.01014928e-04 1.02275826e-01\n",
      " 1.99204460e-05 7.13150144e-01]\n",
      "reward:  5.182490557337208\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "1\n",
      "[4.6980116e-04 1.8375903e-01 4.0275528e-04 1.0234093e-01 2.0040381e-05\n",
      " 7.1300751e-01]\n",
      "reward:  5.093083418904514\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "5\n",
      "[4.7077207e-04 1.8381882e-01 4.0371640e-04 1.0238902e-01 2.0105783e-05\n",
      " 7.1289760e-01]\n",
      "reward:  4.943657232565822\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "1\n",
      "[4.7231797e-04 1.8392913e-01 4.0532340e-04 1.0247481e-01 2.0218355e-05\n",
      " 7.1269816e-01]\n",
      "reward:  4.861216163725728\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[4.73259104e-04 1.83984488e-01 4.06223058e-04 1.02520965e-01\n",
      " 2.02789852e-05 7.12594688e-01]\n",
      "reward:  4.725197978903886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "5\n",
      "[4.7512111e-04 1.8410963e-01 4.0793180e-04 1.0260053e-01 2.0396590e-05\n",
      " 7.1238643e-01]\n",
      "reward:  4.594055848677578\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[4.7555022e-04 1.8414533e-01 4.0823812e-04 1.0261454e-01 2.0419704e-05\n",
      " 7.1233588e-01]\n",
      "reward:  4.4688277741875595\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[4.7718638e-04 1.8421270e-01 4.0961785e-04 1.0268504e-01 2.0523734e-05\n",
      " 7.1219492e-01]\n",
      "reward:  4.349188504841898\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[4.79386683e-04 1.84298038e-01 4.11605957e-04 1.02791004e-01\n",
      " 2.06656150e-05 7.11999297e-01]\n",
      "reward:  4.234862616690692\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "1\n",
      "[4.8098926e-04 1.8436246e-01 4.1303274e-04 1.0286644e-01 2.0769314e-05\n",
      " 7.1185637e-01]\n",
      "reward:  4.177923779455372\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "5\n",
      "[4.8204436e-04 1.8445602e-01 4.1399579e-04 1.0292031e-01 2.0840695e-05\n",
      " 7.1170682e-01]\n",
      "reward:  4.0737022995019485\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[4.8324754e-04 1.8448271e-01 4.1514752e-04 1.0297874e-01 2.0925338e-05\n",
      " 7.1161926e-01]\n",
      "reward:  3.972750450893754\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[4.8343642e-04 1.8447030e-01 4.1528858e-04 1.0298557e-01 2.0936710e-05\n",
      " 7.1162450e-01]\n",
      "reward:  3.8760646954892617\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[4.8632661e-04 1.8460342e-01 4.1795147e-04 1.0312268e-01 2.1125306e-05\n",
      " 7.1134847e-01]\n",
      "reward:  3.783389331617573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[4.8797744e-04 1.8467686e-01 4.1943119e-04 1.0320583e-01 2.1230739e-05\n",
      " 7.1118861e-01]\n",
      "reward:  3.6945256036658147\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "5\n",
      "[4.9004983e-04 1.8477204e-01 4.2140941e-04 1.0330160e-01 2.1369691e-05\n",
      " 7.1099353e-01]\n",
      "reward:  3.609280717058446\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "3\n",
      "[4.9117295e-04 1.8482690e-01 4.2241329e-04 1.0335166e-01 2.1440852e-05\n",
      " 7.1088642e-01]\n",
      "reward:  3.5933093800050773\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[4.9115351e-04 1.8482412e-01 4.2239236e-04 1.0334732e-01 2.1439717e-05\n",
      " 7.1089351e-01]\n",
      "reward:  3.513913254762277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[4.9232389e-04 1.8486944e-01 4.2333911e-04 1.0340319e-01 2.1518370e-05\n",
      " 7.1079028e-01]\n",
      "reward:  3.4359690905453046\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[4.9500936e-04 1.8499279e-01 4.2572763e-04 1.0353137e-01 2.1696264e-05\n",
      " 7.1053344e-01]\n",
      "reward:  3.3611422700739135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[4.9669779e-04 1.8508349e-01 4.2724086e-04 1.0360502e-01 2.1810105e-05\n",
      " 7.1036571e-01]\n",
      "reward:  3.2892103558531955\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[4.9910956e-04 1.8522643e-01 4.2948779e-04 1.0373155e-01 2.1971735e-05\n",
      " 7.1009153e-01]\n",
      "reward:  3.2200497419529053\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "5\n",
      "[1.9759973e-05 9.6078902e-02 7.5393809e-06 1.2552868e-02 4.7602703e-07\n",
      " 8.9134049e-01]\n",
      "reward:  13.103274955175158\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[3.1310810e-05 1.0410069e-01 1.2424721e-05 1.5044721e-02 8.8536007e-07\n",
      " 8.8080996e-01]\n",
      "reward:  12.88827642627345\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[3.0181413e-05 1.0368765e-01 1.1938078e-05 1.4817200e-02 8.4363802e-07\n",
      " 8.8145220e-01]\n",
      "reward:  12.588089254104714\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[2.97607385e-05 1.03635646e-01 1.17760965e-05 1.47445202e-02\n",
      " 8.26710902e-07 8.81577432e-01]\n",
      "reward:  12.220101297507535\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "1\n",
      "[2.9428982e-05 1.0341789e-01 1.1644668e-05 1.4657288e-02 8.1449059e-07\n",
      " 8.8188303e-01]\n",
      "reward:  11.470715269003344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "5\n",
      "[2.9332565e-05 1.0324503e-01 1.1593075e-05 1.4636906e-02 8.0995756e-07\n",
      " 8.8207638e-01]\n",
      "reward:  11.110443884781649\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "5\n",
      "[2.9486906e-05 1.0344020e-01 1.1682930e-05 1.4648620e-02 8.1859031e-07\n",
      " 8.8186932e-01]\n",
      "reward:  10.7321779981193\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[2.97051574e-05 1.03819616e-01 1.17864192e-05 1.46793462e-02\n",
      " 8.28098678e-07 8.81458700e-01]\n",
      "reward:  10.336348124191588\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[2.9760206e-05 1.0412749e-01 1.1818288e-05 1.4690429e-02 8.3158665e-07\n",
      " 8.8113964e-01]\n",
      "reward:  9.933798319157457\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "5\n",
      "[2.97184524e-05 1.04088068e-01 1.17943155e-05 1.46762850e-02\n",
      " 8.30793113e-07 8.81193340e-01]\n",
      "reward:  9.532817984742724\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[2.9857005e-05 1.0405573e-01 1.1871608e-05 1.4712393e-02 8.3685109e-07\n",
      " 8.8118935e-01]\n",
      "reward:  9.139687099291434\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "5\n",
      "[2.98514260e-05 1.03930734e-01 1.18612315e-05 1.47120785e-02\n",
      " 8.35841377e-07 8.81314635e-01]\n",
      "reward:  8.75881243890875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "1\n",
      "[2.9584324e-05 1.0351289e-01 1.1709270e-05 1.4654111e-02 8.2425606e-07\n",
      " 8.8179076e-01]\n",
      "reward:  8.432938388563755\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[2.9680288e-05 1.0351588e-01 1.1727865e-05 1.4665588e-02 8.2698500e-07\n",
      " 8.8177633e-01]\n",
      "reward:  8.103042198984106\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[2.9343519e-05 1.0320698e-01 1.1567426e-05 1.4606383e-02 8.1276522e-07\n",
      " 8.8214493e-01]\n",
      "reward:  7.786544115914868\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[2.8886965e-05 1.0284507e-01 1.1363637e-05 1.4522855e-02 7.9656769e-07\n",
      " 8.8259095e-01]\n",
      "reward:  7.483984108954693\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[2.8706841e-05 1.0270551e-01 1.1270571e-05 1.4488481e-02 7.8889667e-07\n",
      " 8.8276529e-01]\n",
      "reward:  7.195941950562985\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[2.83536756e-05 1.02472059e-01 1.11192685e-05 1.44210365e-02\n",
      " 7.76107527e-07 8.83066654e-01]\n",
      "reward:  6.922554449341084\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "5\n",
      "[2.7979877e-05 1.0218220e-01 1.0957891e-05 1.4344063e-02 7.6105721e-07\n",
      " 8.8343412e-01]\n",
      "reward:  6.6636347031600565\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "5\n",
      "[2.7668086e-05 1.0204068e-01 1.0830690e-05 1.4280564e-02 7.4987776e-07\n",
      " 8.8363951e-01]\n",
      "reward:  6.418780333424061\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "1\n",
      "[2.7257134e-05 1.0180509e-01 1.0652956e-05 1.4197429e-02 7.3462502e-07\n",
      " 8.8395876e-01]\n",
      "reward:  6.27018380452209\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "5\n",
      "[2.7148717e-05 1.0172855e-01 1.0612859e-05 1.4177395e-02 7.3127319e-07\n",
      " 8.8405555e-01]\n",
      "reward:  6.054561047253497\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[2.71467834e-05 1.01660840e-01 1.06159605e-05 1.41747771e-02\n",
      " 7.31005002e-07 8.84125829e-01]\n",
      "reward:  5.848754772754667\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "1\n",
      "[2.70342680e-05 1.01599135e-01 1.05754689e-05 1.41550181e-02\n",
      " 7.27724512e-07 8.84207487e-01]\n",
      "reward:  5.727026323225555\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[2.70422406e-05 1.01552561e-01 1.05809695e-05 1.41566955e-02\n",
      " 7.27664315e-07 8.84252369e-01]\n",
      "reward:  5.544218790049692\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[2.71115532e-05 1.01590455e-01 1.06225061e-05 1.41707314e-02\n",
      " 7.30267288e-07 8.84200275e-01]\n",
      "reward:  5.369129657747658\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "5\n",
      "[2.7083632e-05 1.0154189e-01 1.0611303e-05 1.4162412e-02 7.2934063e-07\n",
      " 8.8425726e-01]\n",
      "reward:  5.202754150318079\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "1\n",
      "[2.72631678e-05 1.01613380e-01 1.06872385e-05 1.41960615e-02\n",
      " 7.35784681e-07 8.84151816e-01]\n",
      "reward:  5.109523843480585\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "5\n",
      "[2.7313781e-05 1.0166992e-01 1.0707967e-05 1.4205329e-02 7.3785259e-07\n",
      " 8.8408595e-01]\n",
      "reward:  4.9602405908464196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "5\n",
      "[2.7477059e-05 1.0179169e-01 1.0777865e-05 1.4237995e-02 7.4395217e-07\n",
      " 8.8393128e-01]\n",
      "reward:  4.816717374654887\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "5\n",
      "[2.7558268e-05 1.0187958e-01 1.0818177e-05 1.4254979e-02 7.4735095e-07\n",
      " 8.8382638e-01]\n",
      "reward:  4.679958595913065\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[2.7751770e-05 1.0201773e-01 1.0900382e-05 1.4292739e-02 7.5450714e-07\n",
      " 8.8365012e-01]\n",
      "reward:  4.549571379980286\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "5\n",
      "[2.7769727e-05 1.0203931e-01 1.0909049e-05 1.4297112e-02 7.5529499e-07\n",
      " 8.8362426e-01]\n",
      "reward:  4.425226162464911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "5\n",
      "[2.8004903e-05 1.0217045e-01 1.1008350e-05 1.4342523e-02 7.6401074e-07\n",
      " 8.8344711e-01]\n",
      "reward:  4.306598659127031\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[2.81448447e-05 1.02275275e-01 1.10700730e-05 1.43699571e-02\n",
      " 7.69273981e-07 8.83314669e-01]\n",
      "reward:  4.193378094666167\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "5\n",
      "[2.82706915e-05 1.02369405e-01 1.11261797e-05 1.43952388e-02\n",
      " 7.74081172e-07 8.83195162e-01]\n",
      "reward:  4.08526564439552\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "5\n",
      "[2.8436916e-05 1.0248510e-01 1.1200302e-05 1.4428051e-02 7.8047628e-07\n",
      " 8.8304639e-01]\n",
      "reward:  3.98197673189265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "1\n",
      "[2.8479171e-05 1.0251891e-01 1.1219876e-05 1.4436293e-02 7.8213185e-07\n",
      " 8.8300419e-01]\n",
      "reward:  3.9341199949840053\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "5\n",
      "[2.8509039e-05 1.0253130e-01 1.1232124e-05 1.4441535e-02 7.8318368e-07\n",
      " 8.8298666e-01]\n",
      "reward:  3.8396621880770145\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "5\n",
      "[2.8760591e-05 1.0271006e-01 1.1344780e-05 1.4490746e-02 7.9283029e-07\n",
      " 8.8275814e-01]\n",
      "reward:  3.7480804396941623\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "1\n",
      "[2.8917879e-05 1.0280413e-01 1.1414092e-05 1.4519547e-02 7.9877037e-07\n",
      " 8.8263518e-01]\n",
      "reward:  3.705607726780093\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "5\n",
      "[2.8925318e-05 1.0280787e-01 1.1416931e-05 1.4521122e-02 7.9900406e-07\n",
      " 8.8262987e-01]\n",
      "reward:  3.6215563335478187\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "5\n",
      "[2.9126926e-05 1.0294687e-01 1.1510538e-05 1.4560076e-02 8.0673129e-07\n",
      " 8.8245159e-01]\n",
      "reward:  3.539868503731324\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[2.9146093e-05 1.0295671e-01 1.1517388e-05 1.4563919e-02 8.0753557e-07\n",
      " 8.8243794e-01]\n",
      "reward:  3.4614561406654167\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "5\n",
      "[2.93962421e-05 1.03104211e-01 1.16253905e-05 1.46119809e-02\n",
      " 8.17110731e-07 8.82241964e-01]\n",
      "reward:  3.386115212329525\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "5\n",
      "[2.95449572e-05 1.03222206e-01 1.16909805e-05 1.46407122e-02\n",
      " 8.22877439e-07 8.82095039e-01]\n",
      "reward:  3.313691268470865\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[2.9796955e-05 1.0341377e-01 1.1805593e-05 1.4691208e-02 8.3270487e-07\n",
      " 8.8185251e-01]\n",
      "reward:  3.244038433863255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "1\n",
      "[2.9845871e-05 1.0345866e-01 1.1828076e-05 1.4701045e-02 8.3472122e-07\n",
      " 8.8179785e-01]\n",
      "reward:  3.2138291097488114\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "1\n",
      "[2.9825660e-05 1.0344723e-01 1.1820393e-05 1.4697565e-02 8.3392928e-07\n",
      " 8.8181275e-01]\n",
      "reward:  3.184106369910456\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "5\n",
      "[2.98542727e-05 1.03467405e-01 1.18333692e-05 1.47029050e-02\n",
      " 8.35003220e-07 8.81787062e-01]\n",
      "reward:  3.1211840157131623\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "5\n",
      "[9.5674957e-10 1.2271696e-04 5.8269674e-11 2.4594421e-05 2.9250454e-12\n",
      " 9.9985266e-01]\n",
      "reward:  13.162749311976027\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "5\n",
      "[2.3512581e-09 1.8060497e-04 1.6227393e-10 3.9495724e-05 9.2373418e-12\n",
      " 9.9978000e-01]\n",
      "reward:  12.962446618421739\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "5\n",
      "[2.2285251e-09 1.7657466e-04 1.5266942e-10 3.8296577e-05 8.6250521e-12\n",
      " 9.9978513e-01]\n",
      "reward:  12.672936877834081\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[2.1437507e-09 1.7432238e-04 1.4612692e-10 3.7485399e-05 8.2035255e-12\n",
      " 9.9978811e-01]\n",
      "reward:  12.311384748173456\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "5\n",
      "[2.1106743e-09 1.7363997e-04 1.4363757e-10 3.7128517e-05 8.0397407e-12\n",
      " 9.9978918e-01]\n",
      "reward:  11.897120602411439\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "5\n",
      "[2.1012965e-09 1.7317820e-04 1.4316232e-10 3.6942409e-05 7.9852184e-12\n",
      " 9.9978989e-01]\n",
      "reward:  11.448816694068597\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "5\n",
      "[2.1110722e-09 1.7418360e-04 1.4439347e-10 3.7018406e-05 8.0447541e-12\n",
      " 9.9978882e-01]\n",
      "reward:  10.98288463936522\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "5\n",
      "[2.1501239e-09 1.7589929e-04 1.4760944e-10 3.7338639e-05 8.2673512e-12\n",
      " 9.9978679e-01]\n",
      "reward:  10.512676054965773\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "5\n",
      "[2.1600657e-09 1.7650849e-04 1.4827357e-10 3.7431593e-05 8.3408844e-12\n",
      " 9.9978608e-01]\n",
      "reward:  10.04834990720364\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "5\n",
      "[2.1738700e-09 1.7652665e-04 1.4924480e-10 3.7542704e-05 8.4176780e-12\n",
      " 9.9978596e-01]\n",
      "reward:  9.597152836602561\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "5\n",
      "[2.1699484e-09 1.7585968e-04 1.4892269e-10 3.7517384e-05 8.4024922e-12\n",
      " 9.9978656e-01]\n",
      "reward:  9.163889118760647\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "5\n",
      "[2.1499744e-09 1.7485832e-04 1.4702346e-10 3.7328071e-05 8.2986734e-12\n",
      " 9.9978787e-01]\n",
      "reward:  8.751446767609773\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.client import device_lib\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "LOSS_CLIPPING = 0.2\n",
    "\n",
    "class ModelParam:\n",
    "    pass\n",
    "\n",
    "def AI_init():\n",
    "    model_param = ModelParam()\n",
    "    # state and action space\n",
    "    model_param.state_size = 4     # [pos,vel,angle,angular_vel] \n",
    "    model_param.img_height = 240\n",
    "    model_param.img_width = 480\n",
    "    model_param.img_channels = 3\n",
    "    model_param.action_size = 6   # [x+,x-,y+,y-,z+,z-,roll+,roll-,pitch+,pitch-,yaw+,yaw-]\n",
    "    model_param.value_size = 1\n",
    "\n",
    "    # AI hyperparameters\n",
    "    model_param.learning_rate_actor = 0.0001\n",
    "    model_param.learning_rate_critic = 0.0001\n",
    "    model_param.epochs_cnt = 10\n",
    "    model_param.discount_rate = 0.999\n",
    "    model_param.smooth_rate = 0.95\n",
    "    model_param.penalty = -400\n",
    "    model_param.episode_num = 50\n",
    "    model_param.mini_batch_step_size = 50        \n",
    "    model_param.node_num = 256     # used after flatten\n",
    "\n",
    "    # Debug variables\n",
    "    model_param.moving_avg_size = 20\n",
    "\n",
    "    # model Initialization\n",
    "    model_param.model_actor = build_model_actor(model_param)\n",
    "    model_param.model_critic = build_model_critic(model_param)\n",
    "\n",
    "    # reward buffer\n",
    "    model_param.reward_list= []\n",
    "    model_param.count_list = []\n",
    "    model_param.moving_avg_list = []\n",
    "\n",
    "    model_param.states, model_param.states_next, model_param.action_matrixs = [],[],[]\n",
    "    model_param.dones, model_param.action_probs, model_param.rewards = [],[],[]\n",
    "\n",
    "    # dummy data\n",
    "    model_param.DUMMY_ACTION_MATRIX = np.zeros((1,1,model_param.action_size))\n",
    "    model_param.DUMMY_ADVANTAGE = np.zeros((1,1,model_param.value_size))\n",
    "    return model_param\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages = in_datas[0], in_datas[1], in_datas[2]\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self([states, action_matrixs, advantages], training=True)\n",
    "                new_policy = K.max(action_matrixs*y_pred, axis=-1)   \n",
    "                old_policy = K.max(action_matrixs*out_action_probs, axis=-1)   \n",
    "                r = new_policy/(old_policy + 1e-8)\n",
    "                clipped = K.clip(r, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)\n",
    "                loss = -K.minimum(r*advantages, clipped*advantages)\n",
    "            \n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            return {\"loss\": loss}\n",
    "        \n",
    "def build_model_actor(model_param):\n",
    "     # 1) The main input: an RGB image\n",
    "    input_states = Input(shape=(model_param.img_height, model_param.img_width, model_param.img_channels),\n",
    "                         name='input_states')\n",
    "\n",
    "    # 2) Additional inputs (same as your old code):\n",
    "    input_action_matrixs = Input(shape=(1, model_param.action_size), name='input_action_matrixs')\n",
    "    input_advantages = Input(shape=(1, model_param.value_size), name='input_advantages')\n",
    "\n",
    "    # 3) Build a small CNN\n",
    "    x = Conv2D(32, kernel_size=8, strides=4, activation='relu')(input_states)\n",
    "    x = Conv2D(64, kernel_size=4, strides=2, activation='relu')(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(model_param.node_num, activation='relu')(x)\n",
    "\n",
    "    # 4) Output: policy over model_param.action_size\n",
    "    out_actions = Dense(model_param.action_size, activation='softmax', name='output')(x)\n",
    "\n",
    "    # 5) Create a MyModel with multi-input\n",
    "    model = MyModel(inputs=[input_states, input_action_matrixs, input_advantages],\n",
    "                    outputs=out_actions)\n",
    "    model.compile(optimizer=Adam(learning_rate=model_param.learning_rate_actor))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_model_critic(model_param):\n",
    "    # Takes image input: (height, width, channels)\n",
    "    input_states = Input(shape=(model_param.img_height, model_param.img_width, model_param.img_channels),\n",
    "                         name='input_states')\n",
    "\n",
    "    x = Conv2D(32, kernel_size=8, strides=4, activation='relu')(input_states)\n",
    "    x = Conv2D(64, kernel_size=4, strides=2, activation='relu')(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(model_param.node_num, activation='relu')(x)\n",
    "\n",
    "    out_values = Dense(model_param.value_size, activation='linear', name='output')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "    model.compile(optimizer=Adam(learning_rate=model_param.learning_rate_critic),\n",
    "                  loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def norm(pointA, pointB):\n",
    "    return np.sqrt((pointA[0] - pointB[0])**2 + (pointA[1] - pointB[1])**2 + (pointA[2] - pointB[2])**2)\n",
    "\n",
    "def get_state(prv_states):\n",
    "    # init states[4]\n",
    "    dt = 1/240\n",
    "    states = np.zeros(4)\n",
    "    target_position = specimen_position+ring_offset\n",
    "    #print(target_position)\n",
    "    end_effector_position = tool.get_tool_pose()[0]+np.array([0.135,0,0])\n",
    "    #print(end_effector_position)\n",
    "    end_effector_orientation = p.getEulerFromQuaternion(tool.get_tool_pose()[1]) + np.array([-np.pi/2,0,-np.pi/2])\n",
    "    states[0] = norm(end_effector_position,target_position) # pos diff\n",
    "    states[1] = (prv_states[0]-states[0])/dt # velocity\n",
    "    states[2] = norm(end_effector_orientation,specimen_orientation_euler)# angle diff\n",
    "    states[3] = (prv_states[2]-states[2])/dt # angular velocity\n",
    "    #print states in a row\n",
    "    print(\"pos diff: \",states[0],\" velocity: \",states[1],\" angle diff: \",states[2],\" angular velocity: \",states[3])\n",
    "    print(\"angle: \",end_effector_orientation, \"angle_specimen\"  ,specimen_orientation_euler)\n",
    "    return states\n",
    "\n",
    "def train(model_param):\n",
    "    for episode in range(model_param.episode_num):\n",
    "        #initialize()\n",
    "        #model_param.states = np.zeros(4)\n",
    "        #model_param.states = get_state(model_param.states)\n",
    "        count, reward_tot = make_memory(model_param)\n",
    "        train_mini_batch(model_param)\n",
    "        clear_memory(model_param)\n",
    "\n",
    "        if count < model_param.episode_num:\n",
    "            reward_tot = reward_tot-model_param.penalty\n",
    "\n",
    "        model_param.reward_list.append(reward_tot)\n",
    "        model_param.count_list.append(count)\n",
    "        model_param.moving_avg_list.append(moving_avg(model_param.count_list,model_param.moving_avg_size))                \n",
    "            \n",
    "        if(episode % 10 == 0):\n",
    "            print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, model_param.moving_avg_list[-1], np.mean(model_param.reward_list)))\n",
    "    \n",
    "    model_param.save_model()\n",
    "def make_memory(model_param):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        base_pose = np.array([0, 0, 0.5])\n",
    "        reward = np.zeros(model_param.value_size)\n",
    "        #advantage = np.zeros(model_param.value_size)\n",
    "        #target = np.zeros(model_param.value_size)\n",
    "        action_matrix = np.zeros(model_param.action_size)\n",
    "        action_unit = np.zeros(model_param.action_size)\n",
    "        done = False\n",
    "        initialize()\n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            #state_t = np.reshape(state,[1, 1, model_param.state_size])\n",
    "            #action_matrix_t = np.reshape(action_matrix,[1, 1, model_param.action_size])\n",
    "\n",
    "            # 1. take a picture\n",
    "            img = camera.get_image()             # 카메라에서 이미지 획득\n",
    "            img = img.astype(np.float32) / 255.0   # float32 변환 및 0~1 정규화\n",
    "            rgb = img[:, :, :3]\n",
    "            rgb = np.expand_dims(rgb, axis=0)      # (1, H, W, 3) 모양으로 확장\n",
    "            model_param.states.append(rgb)\n",
    "            \n",
    "            # 2. calculate probabilty and predict action\n",
    "            action_prob = model_param.model_actor.predict([rgb, model_param.DUMMY_ACTION_MATRIX, model_param.DUMMY_ADVANTAGE])[0]\n",
    "            action = np.random.choice(model_param.action_size, 1, p=action_prob)[0]\n",
    "            action_matrix = np.zeros(model_param.action_size) #초기화\n",
    "            action_matrix[action] = 1 # 하나를 on 으로 만듬\n",
    "            print(action)\n",
    "            print(action_prob)\n",
    "            \n",
    "            # 3. move\n",
    "            # 3.1 robot move\n",
    "            action_unit = [0.01,-0.01,0.01,-0.01,0.01,-0.01] # +- 순서로 액션 을 정함\n",
    "            # action 위치 외 나머지는 0으로 초기화\n",
    "            action_vector = np.zeros_like(action_unit)\n",
    "            action_vector[action] = action_unit[action]  # 선택된 액션만 값을 유지\n",
    "\n",
    "            # 선택된 action에 따라 위치 업데이트 (회전은 별도로 처리)\n",
    "            action_pose = np.array(tool.get_tool_pose()[0])\n",
    "            if action in [0, 1]:\n",
    "                # x축 변경\n",
    "                action_pose[0] += action_unit[action]\n",
    "            elif action in [2, 3]:\n",
    "                # y축 변경\n",
    "                action_pose[1] += action_unit[action]\n",
    "            elif action in [4, 5]:\n",
    "                # z축 변경\n",
    "                action_pose[2] += action_unit[action]\n",
    "            \n",
    "            \n",
    "               \n",
    "            # 3.2 camera move    \n",
    "            for i in range(10): # 10번 반복\n",
    "                tool.set_tool_pose(action_pose, p.getQuaternionFromEuler(base_orientation)) # 액션을 적용\n",
    "                camera_orientation = tool.get_tool_pose()[1] + p.getQuaternionFromEuler([1.57, 0, 1.57]) \n",
    "                camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0])\n",
    "                update_camera(camera_position, camera_orientation)    # 카메라 위치 업데이트\n",
    "                p.stepSimulation()# step simulation\n",
    "           #p.stepSimulation()# step simulation\n",
    "\n",
    "            img_next = camera.get_image()  # 액션 이후 이미지 상태\n",
    "            img_next_uint8 = img_next.astype(np.uint8)\n",
    "            rgb_next = img_next_uint8[:, :, :3]\n",
    "            rgb_next = np.expand_dims(rgb_next, axis=0)\n",
    "\n",
    "            # 다음 이미지 상태를 states_next에 저장\n",
    "            model_param.states_next.append(rgb_next)\n",
    "\n",
    "            # 3.3 check collision\n",
    "            is_collision = check_collision(visualize=False)\n",
    "\n",
    "            # 4. calculate reward\n",
    "            end_effector_position = tool.get_tool_pose()[0]+np.array([0.135,0,0])\n",
    "            specimen_position = np.array([1.0, 0, 1.30])\n",
    "            ring_offset = np.array([0, 0, 0.015])\n",
    "            target_position = specimen_position+ring_offset\n",
    "            judging_criterion = norm(end_effector_position,target_position) # pos difference\n",
    "            pos_reward = 1/judging_criterion\n",
    "            angle_reward = 0\n",
    "            if is_collision:\n",
    "                collision_reward = model_param.penalty\n",
    "            else:\n",
    "                collision_reward = 0\n",
    "\n",
    "            reward = pos_reward + angle_reward + collision_reward\n",
    "            print(\"reward: \", reward)\n",
    "            # 5. judge if finished or not\n",
    "            \n",
    "            if judging_criterion < 0.02:\n",
    "                reward = reward - model_param.penalty\n",
    "                done = True\n",
    "           \n",
    "                               \n",
    "            #model_param.states.append(np.reshape(state_t, [1,model_param.state_size]))\n",
    "            #model_param.states_next.append(np.reshape(state_next_t, [1,model_param.state_size]))\n",
    "            model_param.action_matrixs.append(np.reshape(action_matrix, [1,model_param.action_size]))\n",
    "            model_param.dones.append(np.reshape(0 if done else 1, [1,model_param.value_size]))\n",
    "            model_param.action_probs.append(np.reshape(action_prob, [1,model_param.action_size]))\n",
    "            model_param.rewards.append(np.reshape(reward, [1,model_param.value_size]))\n",
    "            \n",
    "            if(count % model_param.mini_batch_step_size == 0):\n",
    "                train_mini_batch(model_param)\n",
    "                clear_memory(model_param)\n",
    "                initialize()\n",
    "\n",
    "            reward_tot += reward\n",
    "            \n",
    "        return count, reward_tot\n",
    "\n",
    "def make_gae(values, values_next, rewards, dones, discount_rate, smooth_rate):\n",
    "    delta_adv, delta_tar, adv, target = 0, 0, 0, 0\n",
    "    advantages = np.zeros(np.array(values).shape)\n",
    "    targets = np.zeros(np.array(values).shape)\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        delta_adv = rewards[t] + discount_rate * values_next[t] * dones[t] - values[t]\n",
    "        delta_tar = rewards[t] + discount_rate * values_next[t] * dones[t]\n",
    "        adv = delta_adv + smooth_rate * discount_rate * dones[t] * adv\n",
    "        target = delta_tar + smooth_rate * discount_rate * dones[t] * target\n",
    "        advantages[t] = adv\n",
    "        targets[t] = target\n",
    "    return advantages, targets\n",
    "\n",
    "\n",
    "def train_mini_batch(model_param):\n",
    "    if len(model_param.rewards) == 0:\n",
    "        return\n",
    "\n",
    "    # Convert collected data to numpy arrays\n",
    "    action_matrixs_t = np.array(model_param.action_matrixs)\n",
    "    action_probs_t = np.array(model_param.action_probs)\n",
    "    rewards_t = np.array(model_param.rewards)\n",
    "    dones_t = np.array(model_param.dones)\n",
    "\n",
    "    # Use image states directly\n",
    "    states_t = np.vstack(model_param.states)       # (batch_size, H, W, C)\n",
    "    states_next_t = np.vstack(model_param.states_next)\n",
    "\n",
    "    # Predict values from critic\n",
    "    values = model_param.model_critic.predict(states_t)\n",
    "    values_next = model_param.model_critic.predict(states_next_t)\n",
    "\n",
    "    # Calculate advantages and targets using GAE\n",
    "    advantages_t, targets_t = make_gae(values, values_next, rewards_t, dones_t,\n",
    "                                       model_param.discount_rate, model_param.smooth_rate)\n",
    "\n",
    "    # Train actor and critic\n",
    "    states_t = np.array(states_t, dtype=np.float32)\n",
    "    action_matrixs_t = np.array(action_matrixs_t, dtype=np.float32)\n",
    "    advantages_t = np.array(advantages_t, dtype=np.float32)\n",
    "    action_probs_t = np.array(action_probs_t, dtype=np.float32)\n",
    "\n",
    "    # 입력 차원 수정\n",
    "    #action_matrixs_t = np.expand_dims(action_matrixs_t, axis=1)  # (batch_size, 1, action_size)\n",
    "    #advantages_t = np.expand_dims(advantages_t, axis=1)          # (batch_size, 1, value_size)\n",
    "\n",
    "    # PPO Actor 학습\n",
    "    model_param.model_actor.fit([states_t, action_matrixs_t, advantages_t], [action_probs_t],\n",
    "                                epochs=model_param.epochs_cnt, verbose=0)\n",
    "    model_param.model_critic.fit(states_t, targets_t, epochs=model_param.epochs_cnt, verbose=0)\n",
    "\n",
    "\n",
    "def clear_memory(model_param):\n",
    "    model_param.states, model_param.states_next, model_param.action_matrixs = [], [], []\n",
    "    model_param.dones, model_param.action_probs, model_param.rewards = [], [], []\n",
    "\n",
    "def moving_avg(data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "def save_model(model_param):\n",
    "    model_param.model_actor.save(\"./model/ppo_actor\")\n",
    "    model_param.model_critic.save(\"./model/ppo_critic\")\n",
    "    print(\"***** Training Complete and Model Saved *****\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'model_param' in locals():\n",
    "        del model_param\n",
    "\n",
    "    model_param = AI_init()\n",
    "    train(model_param)\n",
    "    save_model(model_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_unit = np.zeros(model_param.action_size) # 12\n",
    "action_unit= [1,1,1,1,1,1,1,1,1,1,1,1] \n",
    "\n",
    "initialize()\n",
    "base_pose = np.array([0, 0, 0.5])\n",
    "action_pose = tool.get_tool_pose()[0] \n",
    "for _ in range(24 * 2):\n",
    "    tool.set_tool_pose(action_pose, p.getQuaternionFromEuler(base_orientation))\n",
    "    p.stepSimulation()\n",
    "\n",
    "camera_orientation = p.getQuaternionFromEuler([1.57, 0, 1.57])\n",
    "camera_position = tool.get_tool_pose()[0] +  np.array([0, 0, 0])\n",
    "update_camera(camera_position, camera_orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50199083, 0.48733655, 0.51342403, 0.49689441])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tool.get_tool_pose()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_model_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │ input_states[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_states        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m24\u001b[0m)     │        \u001b[38;5;34m120\u001b[0m │ input_states[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_action_matri… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_advantages    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)      │         \u001b[38;5;34m50\u001b[0m │ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> (680.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m170\u001b[0m (680.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> (680.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m170\u001b[0m (680.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_states (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m24\u001b[0m)          │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │            \u001b[38;5;34m25\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> (580.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m145\u001b[0m (580.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> (580.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m145\u001b[0m (580.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 203\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    202\u001b[0m     agent \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[1;32m--> 203\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 93\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m state \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mmax_episode_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m---> 93\u001b[0m count, reward_tot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_mini_batch()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_memory()\n",
      "Cell \u001b[1;32mIn[18], line 129\u001b[0m, in \u001b[0;36mAgent.make_memory\u001b[1;34m(self, episode, state)\u001b[0m\n\u001b[0;32m    126\u001b[0m action_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size) \u001b[38;5;66;03m#초기화\u001b[39;00m\n\u001b[0;32m    127\u001b[0m action_matrix[action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 129\u001b[0m state_next, reward, done, none, none2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m state_next_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state_next,[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_size])\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[0;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[1;32mc:\\Users\\b34b3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\__init__.py:424\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.client import device_lib\n",
    "import gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "LOSS_CLIPPING = 0.2\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.node_num = 24\n",
    "        self.learning_rate_actor = 0.0005\n",
    "        self.learning_rate_critic = 0.0005\n",
    "        self.epochs_cnt = 5\n",
    "        self.model_actor = self.build_model_actor()\n",
    "        self.model_critic = self.build_model_critic()\n",
    "        \n",
    "        self.discount_rate = 0.98\n",
    "        self.smooth_rate = 0.95\n",
    "        self.penalty = -400\n",
    "        \n",
    "        self.episode_num = 500\n",
    "        self.mini_batch_step_size = 32        \n",
    "        \n",
    "        self.moving_avg_size = 20\n",
    "        self.reward_list= []\n",
    "        self.count_list = []\n",
    "        self.moving_avg_list = []\n",
    "        \n",
    "        self.states, self.states_next, self.action_matrixs = [],[],[]\n",
    "        self.dones, self.action_probs, self.rewards = [],[],[]\n",
    "        self.DUMMY_ACTION_MATRIX = np.zeros((1,1,self.action_size))\n",
    "        self.DUMMY_ADVANTAGE = np.zeros((1,1,self.value_size))\n",
    "        \n",
    "    class MyModel(tf.keras.Model):\n",
    "        def train_step(self, data):\n",
    "            in_datas, out_action_probs = data\n",
    "            states, action_matrixs, advantages = in_datas[0], in_datas[1], in_datas[2]\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(states, training=True)\n",
    "                new_policy = K.max(action_matrixs*y_pred, axis=-1)   \n",
    "                old_policy = K.max(action_matrixs*out_action_probs, axis=-1)   \n",
    "                r = new_policy/(old_policy)\n",
    "                clipped = K.clip(r, 1-LOSS_CLIPPING, 1+LOSS_CLIPPING)\n",
    "                loss = -K.minimum(r*advantages, clipped*advantages)\n",
    "            \n",
    "            trainable_vars = self.trainable_variables\n",
    "            gradients = tape.gradient(loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "            \n",
    "    def build_model_actor(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        input_action_matrixs = Input(shape=(1,self.action_size), name='input_action_matrixs')\n",
    "        input_advantages = Input(shape=(1,self.value_size), name='input_advantages')\n",
    "\n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='relu')(x)\n",
    "        out_actions = Dense(self.action_size, activation='softmax', name='output')(x)\n",
    "        \n",
    "        model = self.MyModel(inputs=[input_states, input_action_matrixs, input_advantages], outputs=out_actions)\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate_actor))\n",
    "        \n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def build_model_critic(self):\n",
    "        input_states = Input(shape=(1,self.state_size), name='input_states')\n",
    "        x = (input_states)\n",
    "        x = Dense(self.node_num, activation='relu')(x)\n",
    "        out_values = Dense(self.value_size, activation='linear', name='output')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=[input_states], outputs=[out_values])\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate_critic),\n",
    "                      loss='mean_squared_error'\n",
    "                     )\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.episode_num):\n",
    "\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            self.env.max_episode_steps = 500\n",
    "\n",
    "            count, reward_tot = self.make_memory(episode, state)\n",
    "            self.train_mini_batch()\n",
    "            self.clear_memory()\n",
    "            \n",
    "            if count < 500:\n",
    "                reward_tot = reward_tot-self.penalty\n",
    "                \n",
    "            self.reward_list.append(reward_tot)\n",
    "            self.count_list.append(count)\n",
    "            self.moving_avg_list.append(self.moving_avg(self.count_list,self.moving_avg_size))                \n",
    "            \n",
    "            if(episode % 10 == 0):\n",
    "                print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, self.moving_avg_list[-1], np.mean(self.reward_list)))\n",
    "        self.save_model()\n",
    "        \n",
    "    def make_memory(self, episode, state):\n",
    "        reward_tot = 0\n",
    "        count = 0\n",
    "        reward = np.zeros(self.value_size)\n",
    "        advantage = np.zeros(self.value_size)\n",
    "        target = np.zeros(self.value_size)\n",
    "        action_matrix = np.zeros(self.action_size)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            count+=1\n",
    "\n",
    "            state_t = np.reshape(state,[1, 1, self.state_size])\n",
    "            action_matrix_t = np.reshape(action_matrix,[1, 1, self.action_size])\n",
    "            \n",
    "            action_prob = self.model_actor.predict([state_t, self.DUMMY_ACTION_MATRIX, self.DUMMY_ADVANTAGE])\n",
    "\n",
    "            action = np.random.choice(self.action_size, 1, p=action_prob[0][0])[0]\n",
    "            action_matrix = np.zeros(self.action_size) #초기화\n",
    "            action_matrix[action] = 1\n",
    "\n",
    "            state_next, reward, done, none, none2 = self.env.step(action)\n",
    "            \n",
    "            state_next_t = np.reshape(state_next,[1, 1, self.state_size])\n",
    "            \n",
    "            if count < 500 and done:\n",
    "                reward = self.penalty \n",
    "        \n",
    "            self.states.append(np.reshape(state_t, [1,self.state_size]))\n",
    "            self.states_next.append(np.reshape(state_next_t, [1,self.state_size]))\n",
    "            self.action_matrixs.append(np.reshape(action_matrix, [1,self.action_size]))\n",
    "            self.dones.append(np.reshape(0 if done else 1, [1,self.value_size]))\n",
    "            self.action_probs.append(np.reshape(action_prob, [1,self.action_size]))\n",
    "            self.rewards.append(np.reshape(reward, [1,self.value_size]))\n",
    "            \n",
    "            if(count % self.mini_batch_step_size == 0):\n",
    "                self.train_mini_batch()\n",
    "                self.clear_memory()\n",
    "\n",
    "            reward_tot += reward\n",
    "            state = state_next\n",
    "            \n",
    "        return count, reward_tot\n",
    "        \n",
    "    def make_gae(self, values, values_next, rewards, dones):\n",
    "        delta_adv, delta_tar, adv, target = 0, 0, 0, 0\n",
    "        advantages = np.zeros(np.array(values).shape)\n",
    "        targets = np.zeros(np.array(values).shape)\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            delta_adv = rewards[t] + self.discount_rate * values_next[t] * dones[t] - values[t]\n",
    "            delta_tar = rewards[t] + self.discount_rate * values_next[t] * dones[t]\n",
    "            adv = delta_adv + self.smooth_rate*self.discount_rate * dones[t] * adv\n",
    "            target = delta_tar + self.smooth_rate*self.discount_rate * dones[t] * target\n",
    "            advantages[t] = adv\n",
    "            targets[t] = target\n",
    "        return advantages, targets\n",
    "\n",
    "    def train_mini_batch(self):\n",
    "        \n",
    "        if len(self.states) == 0:\n",
    "            return\n",
    "        \n",
    "        states_t = np.array(self.states)\n",
    "        states_next_t = np.array(self.states_next)\n",
    "        action_matrixs_t = np.array(self.action_matrixs)\n",
    "        action_probs_t = np.array(self.action_probs)\n",
    "        rewards_t = np.array(self.rewards)\n",
    "\n",
    "        values = self.model_critic.predict(states_t)\n",
    "        values_next = self.model_critic.predict(states_next_t)\n",
    "        \n",
    "        advantages, targets = self.make_gae(values, values_next, self.rewards, self.dones)\n",
    "        advantages_t = np.array(advantages)\n",
    "        targets_t = np.array(targets)\n",
    "\n",
    "        self.model_actor.fit([states_t, action_matrixs_t, advantages_t], [action_probs_t], epochs=self.epochs_cnt, verbose=0)\n",
    "        self.model_critic.fit(states_t, targets_t, epochs=self.epochs_cnt, verbose=0)       \n",
    " \n",
    "    def clear_memory(self):\n",
    "        self.states, self.states_next, self.action_matrixs = [],[],[]\n",
    "        self.dones, self.action_probs, self.rewards = [],[],[]\n",
    "        \n",
    "    def moving_avg(self, data, size=10):\n",
    "        if len(data) > size:\n",
    "            c = np.array(data[len(data)-size:len(data)]) \n",
    "        else:\n",
    "            c = np.array(data) \n",
    "        return np.mean(c)\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.model_actor.save(\"./model/ppo\")\n",
    "        print(\"*****end learing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.observation_space.shape[0] # 위치, 속도, 각도, 각속도?\n",
    "env.action_space.n # 좌우 2개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get distance between pointA and pointB (3dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos diff:  0.16357298126706452  velocity:  0.0  angle diff:  0.02465193761612129  angular velocity:  0.0\n",
      "angle:  [-0.02178325 -0.01130558 -0.00232204] angle_specimen [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "if 'state' not in locals():\n",
    "    state =np.zeros(4)\n",
    "\n",
    "state = get_state(state)\n",
    "\n",
    "\n",
    "# if variable state does not exsit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
